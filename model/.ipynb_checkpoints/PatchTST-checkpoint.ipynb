{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchTST Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will develop the PatchTST model to predict S&P Close, Dow Jones Close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Congfiguration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will configure the PatchTST model based on the `Economic_Data_1994_2025` dataset we processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PatchTSTConfig, PatchTSTForPrediction, PatchTSTForPretraining\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For faster development\n",
    "device = torch.device('mps')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/Economic_Data_1994-2025.csv')\n",
    "dataset = dataset.drop(['DATE', 'Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding PatchTST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Context Length**\n",
    "\n",
    "    Context length is how far we look back in total. If we were trying to predict the closing price for the SP500 tomorrow, our context length would be how far we look back to make our prediction.\n",
    "\n",
    "- **Patch Length**\n",
    "\n",
    "    Patch length is like a subset of our context length. When looking at our entire context length, patch length is the looking at each individual week up until tomorrow to make our final prediction\n",
    "\n",
    "- **Patch Stride**\n",
    "\n",
    "    Patch stride is how far our patch length will move after observing an individual week. We can overlap weeks to see any comparisons.\n",
    "    \n",
    "For each batch we will pass N amount of rows. Each row has previous rows (context length) attached to it. For each row & it's context length we pass it into our model to train on. During the training process we will used. masked forecasting. This will mask the last portion of our patch's for the model to predict. It then check's it's guesses and updates its weights accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreTraining Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-training model will learn ***every*** column in our dataset from all dates. This will help the model develop relationships between all variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features we are including \n",
    "NUM_INPUT = len(dataset.columns)\n",
    "\n",
    "# Batch size for training\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# For What we are predicting\n",
    "NUM_TARGET = 2\n",
    "\n",
    "# How many steps we take in the context length\n",
    "CONTEXT_LEN = 190\n",
    "\n",
    "# How many steps we take in the context length\n",
    "PATCH_LEN = 10\n",
    "\n",
    "# How far we move our patch length\n",
    "PATCH_STRD = 8\n",
    "\n",
    "# How our model is trained\n",
    "MASK_TYPE = 'forecast'\n",
    "\n",
    "# How many patches are masked for prediction during training\n",
    "\n",
    "NUM_ATT_HEADS = 8\n",
    "\n",
    "# How many days to predict into the future\n",
    "PRED_LEN = int(365 / 4)\n",
    "\n",
    "# Configuring Model\n",
    "pretrain_config = PatchTSTConfig(\n",
    "    num_input_channels = NUM_INPUT,\n",
    "    context_length = CONTEXT_LEN,\n",
    "    patch_length = PATCH_LEN,\n",
    "    stride = PATCH_STRD,\n",
    "    mask_type='forecast',\n",
    "    num_forecast_mask_patches = [int(BATCH_SIZE * .2)], # 20% of our batch\n",
    "    \n",
    ")\n",
    "\n",
    "pretrain_model = PatchTSTForPretraining(pretrain_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are splitting up our data into 2 portions. A `test` set and a `train` set. The test set is used to evalute our model based on training from the train set\n",
    "\n",
    "We split it 80/10/10, where 80% of our data is training data, and 10% of our data is testing, and 10% is validation for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up constraints for development\n",
    "num_train = int(len(dataset) * .8)\n",
    "num_test = int(len(dataset) * .1)\n",
    "num_val = int(len(dataset) * .1)\n",
    "\n",
    "# Breaking up the data into train/test sets.\n",
    "train = dataset[0: num_train]\n",
    "test = dataset[num_train:num_test + num_train]\n",
    "val = dataset[num_test+num_train:(num_test+num_train) + num_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion here grabs context windows for all rows in our train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a context window for each data point to feed into the model during training\n",
    "def create_sequence_windows(data, window_size):\n",
    "    windows = []\n",
    "    \n",
    "    # We start in the dataFrame at an index 'window_size' and look back depending on the window size\n",
    "    # We will grab a context window for all data points\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        windows.append(data.iloc[i:i+window_size].values)\n",
    "    return np.array(windows)\n",
    "\n",
    "train_windows = create_sequence_windows(train, CONTEXT_LEN)\n",
    "test_windows = create_sequence_windows(test, CONTEXT_LEN)\n",
    "val_windows = create_sequence_windows(val, CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts our data in PyTorch tensors for proper data types during training\n",
    "past_values_train = torch.tensor(train_windows, dtype=torch.float32)\n",
    "past_values_test = torch.tensor(test_windows, dtype=torch.float32)\n",
    "past_values_val = torch.tensor(val_windows, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the tensors in a dataset for the dataloader to properly use\n",
    "data_train = TensorDataset(past_values_train)\n",
    "\n",
    "# Divides our data into batches based on the BATCH_SIZE\n",
    "dataloader_train = DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "data_test = TensorDataset(past_values_test)\n",
    "dataloader_test = DataLoader(data_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "data_val = TensorDataset(past_values_val)\n",
    "dataloader_val = DataLoader(data_val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = pretrain_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████| 380/380 [03:04<00:00,  2.06it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.18786994352152472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████| 38/38 [00:06<00:00,  5.64it/s, loss=0.0928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 0 : 0.11471409291813248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████| 380/380 [03:01<00:00,  2.10it/s, loss=0.0823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.39369783721079954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████| 38/38 [00:05<00:00,  6.61it/s, loss=0.0785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 1 : 0.1124115552949278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████| 380/380 [02:59<00:00,  2.12it/s, loss=0.0556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.10369532685726882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████| 38/38 [00:05<00:00,  6.46it/s, loss=0.0726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 2 : 0.09929597515024637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████| 380/380 [03:00<00:00,  2.10it/s, loss=0.0584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.08981309481161205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████████████████| 38/38 [00:05<00:00,  6.52it/s, loss=0.0698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 3 : 0.08511232109250207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|███████████████████| 380/380 [03:01<00:00,  2.09it/s, loss=0.0604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.07742584626141348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████████| 38/38 [00:05<00:00,  6.50it/s, loss=0.0595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 4 : 0.07640274035695352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|███████████████████| 380/380 [02:59<00:00,  2.11it/s, loss=0.0434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.06810858739834083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|█████████████████████| 38/38 [00:05<00:00,  6.69it/s, loss=0.0568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 5 : 0.06970560236981041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|███████████████████| 380/380 [03:00<00:00,  2.11it/s, loss=0.0419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.058398038854724485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|█████████████████████| 38/38 [00:05<00:00,  6.55it/s, loss=0.0349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 6 : 0.05085165386921481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|███████████████████| 380/380 [02:58<00:00,  2.13it/s, loss=0.0224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.04695020574880274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|█████████████████████| 38/38 [00:05<00:00,  6.65it/s, loss=0.0292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 7 : 0.035477304169417995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████| 380/380 [02:59<00:00,  2.12it/s, loss=0.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.034364779609696645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████████████████| 38/38 [00:05<00:00,  6.48it/s, loss=0.012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 8 : 0.026010966737215455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████████████| 380/380 [03:01<00:00,  2.09it/s, loss=0.00612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.021633091327538222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████| 38/38 [00:05<00:00,  6.64it/s, loss=0.00979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 9 : 0.020318992595237336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|████████████████████████| 38/38 [00:05<00:00,  6.66it/s, loss=0.0113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for test set : 0.017642615479417145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(pretrain_model.parameters(), lr=.001)\n",
    "epochs = 10\n",
    "\n",
    "# Puts model in train mode\n",
    "pretrain_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Allows for progress bar during training per epoch\n",
    "    loop = tqdm(dataloader_train, leave=True)\n",
    "    losses = []\n",
    "    \n",
    "    for batch in loop:\n",
    "        \n",
    "        # Clears any previous gradient calculations\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Transfers batch onto GPU for faster processing\n",
    "        past_values = batch[0].to(device)\n",
    "        \n",
    "        # Foward pass through our model, generates predictions\n",
    "        outputs = pretrain_model(past_values=past_values)\n",
    "        \n",
    "        # Get's the loss for our predictions (how far off our predictions were)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Calculates which weights contributed to the error of our prediction\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates the optimizer based on the calculations made from loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(\"Mean Training Loss\", np.mean(losses))\n",
    "        \n",
    "    pretrain_model.eval()\n",
    "    losses = []\n",
    "\n",
    "    loop = tqdm(dataloader_val, leave=True)\n",
    "    \n",
    "    for batch in loop:\n",
    "        \n",
    "        past_values = batch[0].to(device)\n",
    "        \n",
    "        outputs = pretrain_model(past_values=past_values)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(f\"Mean Training Loss for validation set on EPOCH {epoch} : {np.mean(losses)}\")\n",
    "\n",
    "pretrain_model.eval()\n",
    "losses = []\n",
    "\n",
    "loop = tqdm(dataloader_test, leave=True)\n",
    "\n",
    "for batch in loop:\n",
    "    \n",
    "    past_values = batch[0].to(device)\n",
    "    \n",
    "    outputs = pretrain_model(past_values=past_values)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    \n",
    "    loop.set_description(f'Test')\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "print(f\"Mean Training Loss for test set : {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pretrain_model.state_dict(), 'pretrain_model_v1.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many features we are including \n",
    "NUM_INPUT = len(dataset.columns)\n",
    "\n",
    "# Batch size for training\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# For What we are predicting\n",
    "NUM_TARGET = 2\n",
    "\n",
    "# How many steps we take in the context length\n",
    "CONTEXT_LEN = 190\n",
    "\n",
    "# How many steps we take in the context length\n",
    "PATCH_LEN = 10\n",
    "\n",
    "# How far we move our patch length\n",
    "PATCH_STRD = 8\n",
    "\n",
    "# How our model is trained\n",
    "MASK_TYPE = 'forecast'\n",
    "\n",
    "# How many patches are masked for prediction during training\n",
    "\n",
    "NUM_ATT_HEADS = 8\n",
    "\n",
    "# How many days to predict into the future\n",
    "PRED_LEN = 30 # One Month Prediction\n",
    "\n",
    "ft_config = PatchTSTConfig(\n",
    "    num_input_channels = NUM_INPUT,\n",
    "    num_targets = NUM_INPUT,\n",
    "    context_length = CONTEXT_LEN,\n",
    "    patch_length = PATCH_LEN,\n",
    "    stride = PATCH_STRD,\n",
    "    prediction_length=PRED_LEN\n",
    ")\n",
    "\n",
    "ft_model = PatchTSTForPrediction(ft_config)\n",
    "\n",
    "# First, load your saved pretrained model\n",
    "pretrained_weights = torch.load('pretrain_model_v1.bin')\n",
    "\n",
    "# Copy weights from the encoder part of the pretrained model\n",
    "# This will transfer only the compatible weights\n",
    "prediction_model_dict = ft_model.state_dict()\n",
    "for name, param in pretrained_weights.items():\n",
    "    if 'encoder' in name:\n",
    "        # The encoder part is usually named like 'encoder.xxx' in both models\n",
    "        if name in prediction_model_dict:\n",
    "            prediction_model_dict[name] = param\n",
    "\n",
    "ft_model.load_state_dict(prediction_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up constraints for development\n",
    "num_train = int(len(dataset) * .7)\n",
    "num_test = int(len(dataset) * .2)\n",
    "num_val = int(len(dataset) * .1)\n",
    "\n",
    "train_targ = dataset[0: num_train]\n",
    "train_feat = dataset[0: num_train]\n",
    "\n",
    "test_targ = dataset[num_train:num_test + num_train]\n",
    "test_feat = dataset[num_train:num_test + num_train]\n",
    "\n",
    "val_targ = dataset[num_test+num_train:(num_test+num_train) + num_val]\n",
    "val_feat = dataset[num_test+num_train:(num_test+num_train) + num_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Target/Input Features**\n",
    "\n",
    "This part is a little odd. \n",
    "\n",
    "- **Input Features**\n",
    "\n",
    "    To get the target features all we need to do is construct a window that looks at the past N amount of days for each data point.\n",
    "    We include the features we want to predict which makes it **Self Supervised**. \n",
    "\n",
    "- **Output Features**\n",
    "\n",
    "    What we are doing is getting the actual targets we want to predict and making a future window for just the 2 features. \n",
    "    In this case we are looking 90 days into the future, or what the model is predicting, and grabbing those values. This is used \n",
    "    for the model to evalute it's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a context window for each data point to feed into the model during training\n",
    "def create_sequence_windows(data, window_size):\n",
    "    windows = []\n",
    "    \n",
    "    # We start in the dataFrame at an index 'window_size' and look back depending on the window size\n",
    "    # We will grab a context window for all data points\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        windows.append(data.iloc[i:i+window_size].values)\n",
    "    return np.array(windows)\n",
    "\n",
    "train_feat_windows = create_sequence_windows(train_feat, CONTEXT_LEN)\n",
    "test_feat_windows = create_sequence_windows(test_feat, CONTEXT_LEN)\n",
    "val_feat_windows = create_sequence_windows(val_feat, CONTEXT_LEN)\n",
    "\n",
    "# Remove values at the end that don't have enough future data\n",
    "train_feat_windows = train_feat_windows[0:len(train_feat_windows) - PRED_LEN]\n",
    "test_feat_windows = test_feat_windows[0:len(test_feat_windows) - PRED_LEN]\n",
    "val_feat_windows = val_feat_windows[0:len(val_feat_windows) - PRED_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets indices for the target variables, starting from where we first start predicting with a full context length\n",
    "# to the last index that will allow for a full prediction\n",
    "train_targ_indices = range(CONTEXT_LEN, len(train_feat) - PRED_LEN + 1)\n",
    "test_targ_indices = range(CONTEXT_LEN, len(test_feat) - PRED_LEN + 1)\n",
    "val_targ_indices = range(CONTEXT_LEN, len(val_feat) - PRED_LEN + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targ_windows = [train_targ.iloc[i:i+PRED_LEN].values for i in train_targ_indices]\n",
    "test_targ_windows = [test_targ.iloc[i:i+PRED_LEN].values for i in test_targ_indices]\n",
    "val_targ_windows = [val_targ.iloc[i:i+PRED_LEN].values for i in val_targ_indices]\n",
    "\n",
    "train_targ_windows = np.array(train_targ_windows)\n",
    "test_targ_windows = np.array(test_targ_windows )\n",
    "val_targ_windows = np.array(val_targ_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1345, 190, 37]), torch.Size([1345, 30, 37]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "past_train = torch.tensor(train_feat_windows, dtype=torch.float32)\n",
    "past_test = torch.tensor(test_feat_windows, dtype=torch.float32)\n",
    "past_val = torch.tensor(val_feat_windows, dtype=torch.float32)\n",
    "\n",
    "future_train = torch.tensor(train_targ_windows, dtype=torch.float32)\n",
    "future_test = torch.tensor(test_targ_windows, dtype=torch.float32)\n",
    "future_val = torch.tensor(val_targ_windows, dtype=torch.float32)\n",
    "past_test.shape, future_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(past_train, future_train)\n",
    "test_data = TensorDataset(past_test, future_test)\n",
    "val_data = TensorDataset(past_val, future_val)\n",
    "\n",
    "dataloader_train = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(val_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "ft_model = ft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/165 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Updates the optimizer based on the calculations made from loss.backward()\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     adam(\n\u001b[1;32m    245\u001b[0m         params_with_grad,\n\u001b[1;32m    246\u001b[0m         grads,\n\u001b[1;32m    247\u001b[0m         exp_avgs,\n\u001b[1;32m    248\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    249\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    250\u001b[0m         state_steps,\n\u001b[1;32m    251\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    252\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    253\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    254\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    255\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    256\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    257\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    258\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    259\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    260\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    261\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    262\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    263\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    264\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m func(\n\u001b[1;32m    877\u001b[0m     params,\n\u001b[1;32m    878\u001b[0m     grads,\n\u001b[1;32m    879\u001b[0m     exp_avgs,\n\u001b[1;32m    880\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    881\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    882\u001b[0m     state_steps,\n\u001b[1;32m    883\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    884\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    885\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    886\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    887\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    888\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    889\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    890\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    891\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    892\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    893\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    894\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    895\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/optim/adam.py:476\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    474\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    478\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(ft_model.parameters(), lr=.00001)\n",
    "epochs = 10\n",
    "\n",
    "# Puts model in train mode\n",
    "ft_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Allows for progress bar during training per epoch\n",
    "    loop = tqdm(dataloader_train, leave=True)\n",
    "    losses = []\n",
    "    \n",
    "    for past_values, future_values in loop:\n",
    "        \n",
    "        # Clears any previous gradient calculations\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Transfers batch onto GPU for faster processing\n",
    "        past_values = past_values.to(device)\n",
    "        future_values = future_values.to(device)\n",
    "        \n",
    "        # Foward pass through our model, generates predictions\n",
    "        outputs = ft_model(past_values=past_values, future_values=future_values)\n",
    "        \n",
    "        # Get's the loss for our predictions (how far off our predictions were)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Calculates which weights contributed to the error of our prediction\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates the optimizer based on the calculations made from loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(\"Mean Training Loss\", np.mean(losses))\n",
    "        \n",
    "    pretrain_model.eval()\n",
    "    losses = []\n",
    "\n",
    "    loop = tqdm(dataloader_val, leave=True)\n",
    "    \n",
    "    for past_values, future_values in loop:\n",
    "        \n",
    "        past_values = past_values.to(device)\n",
    "        future_values = future_values.to(device)\n",
    "        \n",
    "        # Foward pass through our model, generates predictions\n",
    "        outputs = ft_model(past_values=past_values, future_values=future_values)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(f\"Mean Training Loss for validation set on EPOCH {epoch} : {np.mean(losses)}\")\n",
    "\n",
    "pretrain_model.eval()\n",
    "losses = []\n",
    "\n",
    "loop = tqdm(dataloader_test, leave=True)\n",
    "\n",
    "for past_values, future_values in loop:\n",
    "    \n",
    "    past_values = past_values.to(device)\n",
    "    future_values = future_values.to(device)\n",
    "    \n",
    "    # Foward pass through our model, generates predictions\n",
    "    outputs = ft_model(past_values=past_values, future_values=future_values)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    \n",
    "    loop.set_description(f'Test')\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "print(f\"Mean Training Loss for test set : {np.mean(losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Economic_Model_V1.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = PatchTSTForPrediction(config=config)\n",
    "\n",
    "model_eval.load_state_dict(torch.load('Economic_Model_V1.bin'))\n",
    "model_eval.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.iloc[-CONTEXT_LEN:].values  # Last CONTEXT_LEN days\n",
    "x = torch.FloatTensor(features).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = model_eval(x)\n",
    "\n",
    "predictions = predictions.prediction_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_np =predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "df = pd.read_csv('../data/Economic_Data_1994-2025.csv')\n",
    "\n",
    "# Get the last date in your dataset\n",
    "last_date = pd.to_datetime(df['DATE'])\n",
    "last_date = last_date[7822]\n",
    "\n",
    "# Create date range for predictions\n",
    "future_dates = pd.date_range(\n",
    "    start=last_date + timedelta(days=1),\n",
    "    periods=PRED_LEN,\n",
    "    freq='D'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.drop(['DATE', 'Unnamed: 0'], axis=1)\n",
    "\n",
    "pred_df = pd.DataFrame(pred_np, columns=df_copy.columns)\n",
    "\n",
    "pred_df['DATE'] = future_dates\n",
    "\n",
    "result_df = pd.concat([df, pred_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['DATE'] = pd.to_datetime(result_df['DATE'])\n",
    "\n",
    "result_df.to_csv('data_check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_time_frame_x2(year_start, year_end, df, x1, x2):\n",
    "    df_tf = df[(df['DATE'].dt.year >= year_start) & (df['DATE'].dt.year <= year_end)]\n",
    "\n",
    "    plt.plot(df_tf['DATE'], df_tf[x1], label=x1)\n",
    "    plt.plot(df_tf['DATE'], df_tf[x2], label=x2)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_frame_x2(2025, 2025, result_df, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
