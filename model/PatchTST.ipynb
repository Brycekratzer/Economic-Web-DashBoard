{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchTST Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will develop the PatchTST model to predict S&P Close, Dow Jones Close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Congfiguration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will configure the PatchTST model based on the `Economic_Data_1994_2025` dataset we processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PatchTSTConfig, PatchTSTForPrediction, PatchTSTForPretraining\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For faster development\n",
    "device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'^GSPC Close': 0,\n",
       " '^GSPC High': 1,\n",
       " '^GSPC Low': 2,\n",
       " '^GSPC Open': 3,\n",
       " '^GSPC Volume': 4,\n",
       " '^DJI Close': 5,\n",
       " '^DJI High': 6,\n",
       " '^DJI Low': 7,\n",
       " '^DJI Open': 8,\n",
       " '^DJI Volume': 9,\n",
       " '^IXIC Close': 10,\n",
       " '^IXIC High': 11,\n",
       " '^IXIC Low': 12,\n",
       " '^IXIC Open': 13,\n",
       " '^IXIC Volume': 14,\n",
       " 'CPIAUCSL': 15,\n",
       " 'DFF': 16,\n",
       " 'UNRATE': 17,\n",
       " 'GDP': 18,\n",
       " 'WM2NS': 19,\n",
       " 'ICSA': 20,\n",
       " 'JTS2300JOL': 21,\n",
       " 'JTS1000JOL': 22,\n",
       " 'JTSJOL': 23,\n",
       " 'PCE': 24,\n",
       " 'REVOLSL': 25,\n",
       " 'TDSP': 26,\n",
       " 'CDSP': 27,\n",
       " 'T10Y2Y': 28,\n",
       " 'T10YFF': 29,\n",
       " 'INDPRO': 30,\n",
       " 'TCU': 31,\n",
       " 'RSAFS': 32,\n",
       " 'PSAVERT': 33,\n",
       " 'MORTGAGE30US': 34,\n",
       " 'HOUST': 35,\n",
       " 'PI': 36}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/Economic_Data_1994-2025.csv')\n",
    "dataset = dataset.drop(['DATE', 'Unnamed: 0'], axis=1)\n",
    "\n",
    "col_id = {}\n",
    "curr_id = 0\n",
    "for col in dataset:\n",
    "    col_id[col] = curr_id\n",
    "    curr_id = curr_id + 1\n",
    "\n",
    "# Dictionary of what each index represents\n",
    "col_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding PatchTST\n",
    "\n",
    "- **Context Length**\n",
    "\n",
    "    Context length is how far we look back in total. If we were trying to predict the closing price for the SP500 tomorrow, our context length would be how far we look back to make our prediction.\n",
    "\n",
    "- **Patch Length**\n",
    "\n",
    "    Patch length is like a subset of our context length. When looking at our entire context length, patch length is the looking at each individual week up until tomorrow to make our final prediction\n",
    "\n",
    "- **Patch Stride**\n",
    "\n",
    "    Patch stride is how far our patch length will move after observing an individual week. We can overlap weeks to see any comparisons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreTraining Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-training model will learn ***every*** column in our dataset from all dates. This will help the model develop relationships between all variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features we are including \n",
    "NUM_INPUT = len(dataset.columns)\n",
    "\n",
    "# Batch size for training\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# For What we are predicting\n",
    "NUM_TARGET = 2\n",
    "\n",
    "# How many steps we take in the context length\n",
    "CONTEXT_LEN = 365\n",
    "\n",
    "# How many steps we take in the context length\n",
    "PATCH_LEN = 10\n",
    "\n",
    "# How far we move our patch length\n",
    "PATCH_STRD = 5\n",
    "\n",
    "# How our model is trained\n",
    "MASK_TYPE = 'forecast'\n",
    "\n",
    "# How many patches are masked for prediction during training\n",
    "\n",
    "NUM_ATT_HEADS = 8\n",
    "\n",
    "# How many days to predict into the future\n",
    "PRED_LEN = int(365 / 4)\n",
    "\n",
    "# Configuring Model\n",
    "pretrain_config = PatchTSTConfig(\n",
    "    num_input_channels = NUM_INPUT,\n",
    "    context_length = CONTEXT_LEN,\n",
    "    patch_length = PATCH_LEN,\n",
    "    stride = PATCH_STRD,\n",
    "    mask_type='forecast',\n",
    "    num_forecast_mask_patches = [int(BATCH_SIZE * .2)], # 20% of our batch\n",
    "    \n",
    ")\n",
    "\n",
    "pretrain_model = PatchTSTForPretraining(pretrain_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up constraints for development\n",
    "num_train = int(len(dataset) * .8)\n",
    "num_test = int(len(dataset) * .2)\n",
    "\n",
    "# Breaking up the data into train/test/val sets.\n",
    "train = dataset[0: num_train]\n",
    "test = dataset[num_train:num_test + num_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a context window for each data point to feed into the model during training\n",
    "def create_sequence_windows(data, window_size):\n",
    "    windows = []\n",
    "    \n",
    "    # We start in the dataFrame at an index 'window_size' and look back depending on the window size\n",
    "    # We will grab a context window for all data points\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        windows.append(data.iloc[i:i+window_size].values)\n",
    "    return np.array(windows)\n",
    "\n",
    "train_windows = create_sequence_windows(train, CONTEXT_LEN)\n",
    "test_windows = create_sequence_windows(test, CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_values_train = torch.tensor(train_windows, dtype=torch.float32)\n",
    "past_values_test = torch.tensor(test_windows, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = TensorDataset(past_values_train)\n",
    "dataloader_train = DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "data_test = TensorDataset(past_values_test)\n",
    "dataloader_test = DataLoader(data_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = pretrain_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/369 [00:18<1:53:56, 18.58s/it, loss=2.06]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb#X62sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m outputs \u001b[39m=\u001b[39m pretrain_model(past_values\u001b[39m=\u001b[39mpast_values)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb#X62sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb#X62sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb#X62sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb#X62sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loop\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(pretrain_model.parameters(), lr=.001)\n",
    "epochs = 10\n",
    "\n",
    "pretrain_model.train()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loop = tqdm(dataloader_train, leave=True)\n",
    "    losses = []\n",
    "    \n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        past_values = batch[0].to(device)\n",
    "        \n",
    "        outputs = pretrain_model(past_values=past_values)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(\"Mean Training Loss\", np.mean(losses))\n",
    "        \n",
    "    pretrain_model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    for past_values in loop:\n",
    "        \n",
    "        past_values = past_values.to(device)\n",
    "        \n",
    "        outputs = pretrain_model(past_values=past_values)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(\"Mean Training Loss\", np.mean(losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up constraints for development\n",
    "num_train = int(len(dataset) * .7)\n",
    "num_test = int(len(dataset) * .2)\n",
    "num_val = int(len(dataset) * .1)\n",
    "\n",
    "targets = dataset\n",
    "features = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Target/Input Features**\n",
    "\n",
    "This part is a little odd. \n",
    "\n",
    "- **Input Features**\n",
    "\n",
    "    To get the target features all we need to do is construct a window that looks at the past N amount of days for each data point.\n",
    "    We include the features we want to predict which makes it **Self Supervised**. \n",
    "\n",
    "- **Output Features**\n",
    "\n",
    "    What we are doing is getting the actual targets we want to predict and making a future window for just the 2 features. \n",
    "    In this case we are looking 90 days into the future, or what the model is predicting, and grabbing those values. This is used \n",
    "    for the model to evalute it's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a context window for each data point to feed into the model during training\n",
    "def create_sequence_windows(data, window_size):\n",
    "    windows = []\n",
    "    \n",
    "    # We start in the dataFrame at an index 'window_size' and look back depending on the window size\n",
    "    # We will grab a context window for all data points\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        windows.append(data.iloc[i:i+window_size].values)\n",
    "    return np.array(windows)\n",
    "\n",
    "train_windows = create_sequence_windows(train, CONTEXT_LEN)\n",
    "test_windows = create_sequence_windows(test, CONTEXT_LEN)\n",
    "val_windows = create_sequence_windows(test, CONTEXT_LEN)\n",
    "\n",
    "# Remove values at the end that don't have enough future data\n",
    "input_windows = input_windows[0:7459-91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets indices for the target variables, starting from where we first start predicting with a full context length\n",
    "# to the last index that will allow for a full prediction\n",
    "target_indices = range(CONTEXT_LEN, len(features) - PRED_LEN + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_windows = [targets.iloc[i:i+PRED_LEN].values for i in target_indices]\n",
    "target_windows = np.array(target_windows)\n",
    "target_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_values = torch.tensor(input_windows, dtype=torch.float32)\n",
    "future_values = torch.tensor(target_windows, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TensorDataset(past_values, future_values)\n",
    "dataloader = DataLoader(data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    losses = []\n",
    "    \n",
    "    for past_values, future_values in loop:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        past_values = past_values.to(device)\n",
    "        future_values = future_values.to(device)\n",
    "        \n",
    "        outputs = model(past_values=past_values, future_values=future_values)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Economic_Model_V1.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = PatchTSTForPrediction(config=config)\n",
    "\n",
    "model_eval.load_state_dict(torch.load('Economic_Model_V1.bin'))\n",
    "model_eval.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.iloc[-CONTEXT_LEN:].values  # Last CONTEXT_LEN days\n",
    "x = torch.FloatTensor(features).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = model_eval(x)\n",
    "\n",
    "predictions = predictions.prediction_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_np =predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "df = pd.read_csv('../data/Economic_Data_1994-2025.csv')\n",
    "\n",
    "# Get the last date in your dataset\n",
    "last_date = pd.to_datetime(df['DATE'])\n",
    "last_date = last_date[7822]\n",
    "\n",
    "# Create date range for predictions\n",
    "future_dates = pd.date_range(\n",
    "    start=last_date + timedelta(days=1),\n",
    "    periods=PRED_LEN,\n",
    "    freq='D'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.drop(['DATE', 'Unnamed: 0'], axis=1)\n",
    "\n",
    "pred_df = pd.DataFrame(pred_np, columns=df_copy.columns)\n",
    "\n",
    "pred_df['DATE'] = future_dates\n",
    "\n",
    "result_df = pd.concat([df, pred_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['DATE'] = pd.to_datetime(result_df['DATE'])\n",
    "\n",
    "result_df.to_csv('data_check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_time_frame_x2(year_start, year_end, df, x1, x2):\n",
    "    df_tf = df[(df['DATE'].dt.year >= year_start) & (df['DATE'].dt.year <= year_end)]\n",
    "\n",
    "    plt.plot(df_tf['DATE'], df_tf[x1], label=x1)\n",
    "    plt.plot(df_tf['DATE'], df_tf[x2], label=x2)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_frame_x2(2025, 2025, result_df, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Personal_Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
