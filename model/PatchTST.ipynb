{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchTST Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will develop the PatchTST model to predict S&P Close, Dow Jones Close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Congfiguration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will configure the PatchTST model based on the `Economic_Data_1994_2025` dataset we processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PatchTSTConfig, PatchTSTForPrediction, PatchTSTForPretraining\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For faster development\n",
    "device = torch.device('mps')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/Economic_Data_1994-2025.csv')\n",
    "dataset = dataset.drop(['DATE', 'Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding PatchTST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Context Length**\n",
    "\n",
    "    Context length is how far we look back in total. If we were trying to predict the closing price for the SP500 tomorrow, our context length would be how far we look back to make our prediction.\n",
    "\n",
    "- **Patch Length**\n",
    "\n",
    "    Patch length is like a subset of our context length. When looking at our entire context length, patch length is the looking at each individual week up until tomorrow to make our final prediction\n",
    "\n",
    "- **Patch Stride**\n",
    "\n",
    "    Patch stride is how far our patch length will move after observing an individual week. We can overlap weeks to see any comparisons.\n",
    "    \n",
    "For each batch we will pass N amount of rows. Each row has previous rows (context length) attached to it. For each row & it's context length we pass it into our model to train on. During the training process we will used. masked forecasting. This will mask the last portion of our patch's for the model to predict. It then check's it's guesses and updates its weights accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreTraining Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-training model will learn ***every*** column in our dataset from all dates. This will help the model develop relationships between all variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features we are including \n",
    "NUM_INPUT = len(dataset.columns)\n",
    "\n",
    "# Batch size for training\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# For What we are predicting\n",
    "NUM_TARGET = 2\n",
    "\n",
    "# How many steps we take in the context length\n",
    "CONTEXT_LEN = 60\n",
    "\n",
    "# How many steps we take in the context length\n",
    "PATCH_LEN = 5\n",
    "\n",
    "# How far we move our patch length\n",
    "PATCH_STRD = 4\n",
    "\n",
    "# How our model is trained\n",
    "MASK_TYPE = 'forecast'\n",
    "\n",
    "# How many items are masked in our forcast\n",
    "NUM_PATCH = [int(BATCH_SIZE * .2)] # 20% of our batch\n",
    "\n",
    "# Configuring Model\n",
    "pretrain_config = PatchTSTConfig(\n",
    "    num_input_channels = NUM_INPUT,\n",
    "    context_length = CONTEXT_LEN,\n",
    "    patch_length = PATCH_LEN,\n",
    "    stride = PATCH_STRD,\n",
    "    mask_type='forecast',\n",
    "    num_forecast_mask_patches = NUM_PATCH\n",
    "    do_mask_input=False\n",
    ")\n",
    "\n",
    "pretrain_model = PatchTSTForPretraining(pretrain_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are splitting up our data into 2 portions. A `test` set and a `train` set. The test set is used to evalute our model based on training from the train set\n",
    "\n",
    "We split it 80/10/10, where 80% of our data is training data, and 10% of our data is testing, and 10% is validation for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up constraints for development\n",
    "num_train = int(len(dataset) * .8)\n",
    "num_test = int(len(dataset) * .1)\n",
    "num_val = int(len(dataset) * .1)\n",
    "\n",
    "# Breaking up the data into train/test sets.\n",
    "train = dataset[0: num_train]\n",
    "test = dataset[num_train:num_test + num_train]\n",
    "val = dataset[num_test+num_train:(num_test+num_train) + num_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion here grabs context windows for all rows in our train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a context window for each data point to feed into the model during training\n",
    "def create_sequence_windows(data, window_size):\n",
    "    windows = []\n",
    "    \n",
    "    # We start in the dataFrame at an index 'window_size' and look back depending on the window size\n",
    "    # We will grab a context window for all data points\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        windows.append(data.iloc[i:i+window_size].values)\n",
    "    return np.array(windows)\n",
    "\n",
    "train_windows = create_sequence_windows(train, CONTEXT_LEN)\n",
    "test_windows = create_sequence_windows(test, CONTEXT_LEN)\n",
    "val_windows = create_sequence_windows(val, CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts our data in PyTorch tensors for proper data types during training\n",
    "past_values_train = torch.tensor(train_windows, dtype=torch.float32)\n",
    "past_values_test = torch.tensor(test_windows, dtype=torch.float32)\n",
    "past_values_val = torch.tensor(val_windows, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing our data to be passed into our model for pre training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the tensors in a dataset for the dataloader to properly use\n",
    "data_train = TensorDataset(past_values_train)\n",
    "\n",
    "# Divides our data into batches based on the BATCH_SIZE\n",
    "dataloader_train = DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "data_test = TensorDataset(past_values_test)\n",
    "dataloader_test = DataLoader(data_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "data_val = TensorDataset(past_values_val)\n",
    "dataloader_val = DataLoader(data_val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = pretrain_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are training our model by doing the following:\n",
    "\n",
    "1. Divide Training into Epoch\n",
    "    - For each epoch we:\n",
    "        - Set model to train mode\n",
    "        - Pass in all of our data one batch size at a time\n",
    "        - After training on a patch we update our parameters\n",
    "        - Put our model into evaluation mode\n",
    "        - Test on our validation set and print results to output\n",
    "2. Final Train\n",
    "    - Once we pass through all epochs we do a final test on our\n",
    "        never seen data, `test` set. \n",
    "    - We iterate through our data one batch size at a time and evaluate the model\n",
    "        one last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 388/388 [00:50<00:00,  7.69it/s, loss=1.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.8991099428193471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 46/46 [00:01<00:00, 23.42it/s, loss=1.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 0 : 1.0320049809372944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 388/388 [00:49<00:00,  7.88it/s, loss=0.57] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.427272789333899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 46/46 [00:01<00:00, 24.04it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 1 : 0.27811477884002356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 388/388 [00:49<00:00,  7.82it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.3037517006249772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 46/46 [00:01<00:00, 23.98it/s, loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 2 : 0.19602242241735043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 388/388 [00:49<00:00,  7.87it/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.3055087956263847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 46/46 [00:02<00:00, 22.74it/s, loss=0.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 3 : 0.21317156566225964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 388/388 [00:52<00:00,  7.36it/s, loss=42.6]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 3874.3598906028365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 46/46 [00:02<00:00, 22.54it/s, loss=68.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 4 : 44.07240776393724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 388/388 [00:54<00:00,  7.11it/s, loss=21.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 34.014431068577714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 46/46 [00:02<00:00, 22.71it/s, loss=46.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 5 : 28.757690346759297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 388/388 [00:53<00:00,  7.28it/s, loss=16.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 21.668511936345052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 46/46 [00:02<00:00, 19.95it/s, loss=33.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 6 : 19.39630203661711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 388/388 [00:53<00:00,  7.25it/s, loss=11.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 17.70420574035841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 46/46 [00:02<00:00, 22.34it/s, loss=10.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 7 : 9.192035965297533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 388/388 [00:51<00:00,  7.56it/s, loss=10.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 43.4321666501232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 46/46 [00:01<00:00, 23.98it/s, loss=8.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 8 : 5.790601896203083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 388/388 [00:49<00:00,  7.77it/s, loss=62.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 11.196586321924151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 46/46 [00:01<00:00, 23.13it/s, loss=93.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 9 : 80.71389455380647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 46/46 [00:01<00:00, 24.02it/s, loss=76.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for test set : 80.52690555738366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(pretrain_model.parameters(), lr=.001)\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Allows for progress bar during training per epoch\n",
    "    loop = tqdm(dataloader_train, leave=True)\n",
    "    losses = []\n",
    "    \n",
    "    for batch in loop:\n",
    "        \n",
    "        # Puts model in train mode\n",
    "        pretrain_model.train()\n",
    "        \n",
    "        # Clears any previous gradient calculations\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Transfers batch onto GPU for faster processing\n",
    "        past_values = batch[0].to(device)\n",
    "        \n",
    "        # Foward pass through our model, generates predictions\n",
    "        outputs = pretrain_model(past_values=past_values)\n",
    "        \n",
    "        # Get's the loss for our predictions (how far off our predictions were)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Calculates which weights contributed to the error of our prediction\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates the optimizer based on the calculations made from loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(\"Mean Training Loss\", np.mean(losses))\n",
    "        \n",
    "    pretrain_model.eval()\n",
    "    losses = []\n",
    "\n",
    "    loop = tqdm(dataloader_val, leave=True)\n",
    "    \n",
    "    for batch in loop:\n",
    "        pretrain_model.eval()\n",
    "        \n",
    "        past_values = batch[0].to(device)\n",
    "        \n",
    "        outputs = pretrain_model(past_values=past_values)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(f\"Mean Training Loss for validation set on EPOCH {epoch} : {np.mean(losses)}\")\n",
    "\n",
    "pretrain_model.eval()\n",
    "losses = []\n",
    "\n",
    "loop = tqdm(dataloader_test, leave=True)\n",
    "\n",
    "for batch in loop:\n",
    "    \n",
    "    past_values = batch[0].to(device)\n",
    "    \n",
    "    outputs = pretrain_model(past_values=past_values)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    \n",
    "    loop.set_description(f'Test')\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "print(f\"Mean Training Loss for test set : {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pretrain_model.state_dict(), 'pretrain_model_no_mask_v2.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT = len(dataset.columns)\n",
    "BATCH_SIZE = 16\n",
    "CONTEXT_LEN = 190\n",
    "PATCH_LEN = 10\n",
    "PATCH_STRD = 8\n",
    "MASK_TYPE = 'forecast'\n",
    "NUM_PATCH = [int(BATCH_SIZE * .2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pretrained_model_no_mask** \n",
    "\n",
    "    Mean Training Loss for test set : 0.0191\n",
    "\n",
    "- **pretrained_model_mask** \n",
    "\n",
    "    Mean Training Loss for test set : 0.0199\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters Set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT = len(dataset.columns)\n",
    "BATCH_SIZE = 16\n",
    "CONTEXT_LEN = 60 # Only looking back 60 days instead of 190\n",
    "PATCH_LEN = 5 # Look at each prior week before for each data point in context length \n",
    "PATCH_STRD = 4 # Move window while keeping the last week's most recent day in set.\n",
    "MASK_TYPE = 'forecast'\n",
    "NUM_PATCH = [int(BATCH_SIZE * .2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pretrained_model_no_mask** \n",
    "\n",
    "    Mean Training Loss for test set : 80.5\n",
    "\n",
    "- **pretrained_model_mask** \n",
    "\n",
    "    Mean Training Loss for test set : 1.29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchTSTForPrediction(\n",
       "  (model): PatchTSTModel(\n",
       "    (scaler): PatchTSTScaler(\n",
       "      (scaler): PatchTSTStdScaler()\n",
       "    )\n",
       "    (patchifier): PatchTSTPatchify()\n",
       "    (masking): Identity()\n",
       "    (encoder): PatchTSTEncoder(\n",
       "      (embedder): PatchTSTEmbedding(\n",
       "        (input_embedding): Linear(in_features=10, out_features=128, bias=True)\n",
       "      )\n",
       "      (positional_encoder): PatchTSTPositionalEncoding(\n",
       "        (positional_dropout): Identity()\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x PatchTSTEncoderLayer(\n",
       "          (self_attn): PatchTSTAttention(\n",
       "            (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (dropout_path1): Identity()\n",
       "          (norm_sublayer1): PatchTSTBatchNorm(\n",
       "            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (ff): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (1): GELUActivation()\n",
       "            (2): Identity()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (dropout_path3): Identity()\n",
       "          (norm_sublayer3): PatchTSTBatchNorm(\n",
       "            (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): PatchTSTPredictionHead(\n",
       "    (flatten): Flatten(start_dim=2, end_dim=-1)\n",
       "    (projection): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (dropout): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many features we are including \n",
    "NUM_INPUT = len(dataset.columns)\n",
    "\n",
    "# Batch size for training\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# For What we are predicting\n",
    "NUM_TARGET = 2\n",
    "\n",
    "# How many steps we take in the context length\n",
    "CONTEXT_LEN = 190\n",
    "\n",
    "# How many steps we take in the context length\n",
    "PATCH_LEN = 10\n",
    "\n",
    "# How far we move our patch length\n",
    "PATCH_STRD = 8\n",
    "\n",
    "# How many patches are masked for prediction during training\n",
    "NUM_ATT_HEADS = 8\n",
    "\n",
    "# How many days to predict into the future\n",
    "PRED_LEN = 10 # One Month Prediction\n",
    "\n",
    "ft_config = PatchTSTConfig(\n",
    "    num_input_channels = NUM_INPUT,\n",
    "    num_targets = NUM_TARGET,\n",
    "    context_length = CONTEXT_LEN,\n",
    "    patch_length = PATCH_LEN,\n",
    "    stride = PATCH_STRD,\n",
    "    prediction_length=PRED_LEN\n",
    ")\n",
    "\n",
    "ft_model = PatchTSTForPrediction(ft_config)\n",
    "\n",
    "# First, load your saved pretrained model\n",
    "pretrained_weights = torch.load('pretrain_model_no_mask.bin')\n",
    "\n",
    "# Copy weights from the encoder part of the pretrained model\n",
    "# This will transfer only the compatible weights\n",
    "prediction_model_dict = ft_model.state_dict()\n",
    "for name, param in pretrained_weights.items():\n",
    "    if 'encoder' in name:\n",
    "        # The encoder part is usually named like 'encoder.xxx' in both models\n",
    "        if name in prediction_model_dict:\n",
    "            prediction_model_dict[name] = param\n",
    "\n",
    "ft_model.load_state_dict(prediction_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up constraints for development\n",
    "num_train = int(len(dataset) * .7)\n",
    "num_test = int(len(dataset) * .2)\n",
    "num_val = int(len(dataset) * .1)\n",
    "\n",
    "targ_data = dataset[['^GSPC Close', '^IXIC Close']]\n",
    "\n",
    "train_targ = targ_data[0: num_train]\n",
    "train_feat = dataset[0: num_train]\n",
    "\n",
    "test_targ = targ_data[num_train:num_test + num_train]\n",
    "test_feat = dataset[num_train:num_test + num_train]\n",
    "\n",
    "val_targ = targ_data[num_test+num_train:(num_test+num_train) + num_val]\n",
    "val_feat = dataset[num_test+num_train:(num_test+num_train) + num_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Target/Input Features**\n",
    "\n",
    "This part is a little odd. \n",
    "\n",
    "- **Input Features**\n",
    "\n",
    "    To get the target features all we need to do is construct a window that looks at the past N amount of days for each data point.\n",
    "    We include the features we want to predict which makes it **Self Supervised**. \n",
    "\n",
    "- **Output Features**\n",
    "\n",
    "    What we are doing is getting the actual targets we want to predict and making a future window for just the 2 features. \n",
    "    In this case we are looking 90 days into the future, or what the model is predicting, and grabbing those values. This is used \n",
    "    for the model to evalute it's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a context window for each data point to feed into the model during training\n",
    "def create_sequence_windows(data, window_size):\n",
    "    windows = []\n",
    "    \n",
    "    # We start in the dataFrame at an index 'window_size' and look back depending on the window size\n",
    "    # We will grab a context window for all data points\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        windows.append(data.iloc[i:i+window_size].values)\n",
    "    return np.array(windows)\n",
    "\n",
    "train_feat_windows = create_sequence_windows(train_feat, CONTEXT_LEN)\n",
    "test_feat_windows = create_sequence_windows(test_feat, CONTEXT_LEN)\n",
    "val_feat_windows = create_sequence_windows(val_feat, CONTEXT_LEN)\n",
    "\n",
    "# Remove values at the end that don't have enough future data\n",
    "train_feat_windows = train_feat_windows[0:len(train_feat_windows) - PRED_LEN]\n",
    "test_feat_windows = test_feat_windows[0:len(test_feat_windows) - PRED_LEN]\n",
    "val_feat_windows = val_feat_windows[0:len(val_feat_windows) - PRED_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets indices for the target variables, starting from where we first start predicting with a full context length\n",
    "# to the last index that will allow for a full prediction\n",
    "train_targ_indices = range(CONTEXT_LEN, len(train_feat) - PRED_LEN + 1)\n",
    "test_targ_indices = range(CONTEXT_LEN, len(test_feat) - PRED_LEN + 1)\n",
    "val_targ_indices = range(CONTEXT_LEN, len(val_feat) - PRED_LEN + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targ_windows = [train_targ.iloc[i:i+PRED_LEN].values for i in train_targ_indices]\n",
    "test_targ_windows = [test_targ.iloc[i:i+PRED_LEN].values for i in test_targ_indices]\n",
    "val_targ_windows = [val_targ.iloc[i:i+PRED_LEN].values for i in val_targ_indices]\n",
    "\n",
    "train_targ_windows = np.array(train_targ_windows)\n",
    "test_targ_windows = np.array(test_targ_windows )\n",
    "val_targ_windows = np.array(val_targ_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1365, 190, 37]), torch.Size([1365, 10, 2]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "past_train = torch.tensor(train_feat_windows, dtype=torch.float32)\n",
    "past_test = torch.tensor(test_feat_windows, dtype=torch.float32)\n",
    "past_val = torch.tensor(val_feat_windows, dtype=torch.float32)\n",
    "\n",
    "future_train = torch.tensor(train_targ_windows, dtype=torch.float32)\n",
    "future_test = torch.tensor(test_targ_windows, dtype=torch.float32)\n",
    "future_val = torch.tensor(val_targ_windows, dtype=torch.float32)\n",
    "past_test.shape, future_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(past_train, future_train)\n",
    "test_data = TensorDataset(past_test, future_test)\n",
    "val_data = TensorDataset(past_val, future_val)\n",
    "\n",
    "dataloader_train = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(val_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "ft_model = ft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/165 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb Cell 42\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb#Y243sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m future_values \u001b[39m=\u001b[39m future_values\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb#Y243sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Foward pass through our model, generates predictions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb#Y243sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m outputs \u001b[39m=\u001b[39m ft_model(past_values\u001b[39m=\u001b[39mpast_values, future_values\u001b[39m=\u001b[39mfuture_values)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb#Y243sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Get's the loss for our predictions (how far off our predictions were)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/PersonalProjects/Economic-DashBoard/Economic-Web-DashBoard/model/PatchTST.ipynb#Y243sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/transformers/models/patchtst/modeling_patchtst.py:1731\u001b[0m, in \u001b[0;36mPatchTSTForPrediction.forward\u001b[0;34m(self, past_values, past_observed_mask, future_values, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m   1728\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1730\u001b[0m \u001b[39m# get model output\u001b[39;00m\n\u001b[0;32m-> 1731\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m   1732\u001b[0m     past_values\u001b[39m=\u001b[39mpast_values,\n\u001b[1;32m   1733\u001b[0m     past_observed_mask\u001b[39m=\u001b[39mpast_observed_mask,\n\u001b[1;32m   1734\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1735\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1736\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1737\u001b[0m )\n\u001b[1;32m   1738\u001b[0m \u001b[39m# get output head\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead(model_output\u001b[39m.\u001b[39mlast_hidden_state)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/transformers/models/patchtst/modeling_patchtst.py:1238\u001b[0m, in \u001b[0;36mPatchTSTModel.forward\u001b[0;34m(self, past_values, past_observed_mask, future_values, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     masked_values, mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasking(patched_values), \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m encoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1239\u001b[0m     patch_input\u001b[39m=\u001b[39mmasked_values, output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states, output_attentions\u001b[39m=\u001b[39moutput_attentions\n\u001b[1;32m   1240\u001b[0m )\n\u001b[1;32m   1242\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1243\u001b[0m     outputs \u001b[39m=\u001b[39m (encoder_output\u001b[39m.\u001b[39mlast_hidden_state, encoder_output\u001b[39m.\u001b[39mhidden_states, encoder_output\u001b[39m.\u001b[39mattentions)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/transformers/models/patchtst/modeling_patchtst.py:766\u001b[0m, in \u001b[0;36mPatchTSTEncoder.forward\u001b[0;34m(self, patch_input, output_hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    764\u001b[0m     encoder_states \u001b[39m=\u001b[39m encoder_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 766\u001b[0m layer_outputs \u001b[39m=\u001b[39m encoder_layer(hidden_state\u001b[39m=\u001b[39mhidden_state, output_attentions\u001b[39m=\u001b[39moutput_attentions)\n\u001b[1;32m    767\u001b[0m \u001b[39m# get hidden state. hidden_state shape is [bs x num_channels x num_patches x d_model]\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39m# or [bs x num_channels x (num_patches+1) x d_model] if use cls_token\u001b[39;00m\n\u001b[1;32m    769\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/transformers/models/patchtst/modeling_patchtst.py:574\u001b[0m, in \u001b[0;36mPatchTSTEncoderLayer.forward\u001b[0;34m(self, hidden_state, output_attentions)\u001b[0m\n\u001b[1;32m    570\u001b[0m hidden_state \u001b[39m=\u001b[39m hidden_state\u001b[39m.\u001b[39mview(batch_size \u001b[39m*\u001b[39m num_input_channels, sequence_length, d_model)\n\u001b[1;32m    571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_norm:\n\u001b[1;32m    572\u001b[0m     \u001b[39m## Norm and Position-wise Feed-Forward and Add residual connection\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     \u001b[39m# Add: residual connection with residual dropout\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m     hidden_state \u001b[39m=\u001b[39m hidden_state \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_path3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_sublayer3(hidden_state)))\n\u001b[1;32m    575\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[39m## Position-wise Feed-Forward and Add residual connection and Norm\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     \u001b[39m# Add: residual connection with residual dropout\u001b[39;00m\n\u001b[1;32m    578\u001b[0m     hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_sublayer3(hidden_state \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_path3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(hidden_state)))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/transformers/models/patchtst/modeling_patchtst.py:214\u001b[0m, in \u001b[0;36mPatchTSTBatchNorm.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    inputs (`torch.Tensor` of shape `(batch_size, sequence_length, d_model)`):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m    `torch.Tensor` of shape `(batch_size, sequence_length, d_model)`\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m output \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# output: (batch_size, d_model, sequence_length)\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchnorm(output)\n\u001b[1;32m    215\u001b[0m \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m    194\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    195\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean\n\u001b[1;32m    197\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats\n\u001b[1;32m    198\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    202\u001b[0m     bn_training,\n\u001b[1;32m    203\u001b[0m     exponential_average_factor,\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps,\n\u001b[1;32m    205\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/torch/nn/functional.py:2822\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2819\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2820\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2822\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2823\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m   2824\u001b[0m     weight,\n\u001b[1;32m   2825\u001b[0m     bias,\n\u001b[1;32m   2826\u001b[0m     running_mean,\n\u001b[1;32m   2827\u001b[0m     running_var,\n\u001b[1;32m   2828\u001b[0m     training,\n\u001b[1;32m   2829\u001b[0m     momentum,\n\u001b[1;32m   2830\u001b[0m     eps,\n\u001b[1;32m   2831\u001b[0m     torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled,\n\u001b[1;32m   2832\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(ft_model.parameters(), lr=.000001)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Allows for progress bar during training per epoch\n",
    "    loop = tqdm(dataloader_train, leave=True)\n",
    "    losses = []\n",
    "    \n",
    "    for past_values, future_values in loop:\n",
    "        ft_model.train()\n",
    "        \n",
    "        # Clears any previous gradient calculations\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Transfers batch onto GPU for faster processing\n",
    "        past_values = past_values.to(device)\n",
    "        future_values = future_values.to(device)\n",
    "        \n",
    "        # Foward pass through our model, generates predictions\n",
    "        outputs = ft_model(past_values=past_values, future_values=future_values)\n",
    "        \n",
    "        # Get's the loss for our predictions (how far off our predictions were)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Calculates which weights contributed to the error of our prediction\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates the optimizer based on the calculations made from loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(\"Mean Training Loss\", np.mean(losses))\n",
    "        \n",
    "    ft_model.eval()\n",
    "    losses = []\n",
    "\n",
    "    loop = tqdm(dataloader_val, leave=True)\n",
    "    \n",
    "    for past_values, future_values in loop:\n",
    "        \n",
    "        past_values = past_values.to(device)\n",
    "        future_values = future_values.to(device)\n",
    "        \n",
    "        # Foward pass through our model, generates predictions\n",
    "        outputs = ft_model(past_values=past_values, future_values=future_values)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(f\"Mean Training Loss for validation set on EPOCH {epoch} : {np.mean(losses)}\")\n",
    "\n",
    "ft_model.eval()\n",
    "losses = []\n",
    "\n",
    "loop = tqdm(dataloader_test, leave=True)\n",
    "\n",
    "for past_values, future_values in loop:\n",
    "    \n",
    "    past_values = past_values.to(device)\n",
    "    future_values = future_values.to(device)\n",
    "    \n",
    "    # Foward pass through our model, generates predictions\n",
    "    outputs = ft_model(past_values=past_values, future_values=future_values)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    \n",
    "    loop.set_description(f'Test')\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "print(f\"Mean Training Loss for test set : {np.mean(losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Economic_Model_V1.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = PatchTSTForPrediction(config=config)\n",
    "\n",
    "model_eval.load_state_dict(torch.load('Economic_Model_V1.bin'))\n",
    "model_eval.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    features = dataset.iloc[-CONTEXT_LEN:].values  # Last CONTEXT_LEN days\n",
    "    features = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "    predictions = ft_model(past_values=features)\n",
    "    forcast = predictions.prediction_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forcast.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_np = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "df = pd.read_csv('../data/Economic_Data_1994-2025.csv')\n",
    "\n",
    "# Get the last date in your dataset\n",
    "last_date = pd.to_datetime(df['DATE'])\n",
    "last_date = last_date[7822]\n",
    "\n",
    "# Create date range for predictions\n",
    "future_dates = pd.date_range(\n",
    "    start=last_date + timedelta(days=1),\n",
    "    periods=PRED_LEN,\n",
    "    freq='D'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.drop(['DATE', 'Unnamed: 0'], axis=1)\n",
    "\n",
    "pred_df = pd.DataFrame(pred_np, columns=df_copy.columns)\n",
    "\n",
    "pred_df['DATE'] = future_dates\n",
    "\n",
    "result_df = pd.concat([df, pred_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['DATE'] = pd.to_datetime(result_df['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_time_frame_x2(year_start, year_end, df, x1, x2):\n",
    "    df_tf = df[(df['DATE'].dt.year >= year_start) & (df['DATE'].dt.year <= year_end)]\n",
    "\n",
    "    plt.plot(df_tf['DATE'], df_tf[x1], label=x1)\n",
    "    plt.plot(df_tf['DATE'], df_tf[x2], label=x2)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_time_frame_x2(2025, 2025, result_df, '^GSPC Close', '^IXIC Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
