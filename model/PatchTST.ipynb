{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchTST Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will develop the PatchTST model to predict S&P Close, Dow Jones Close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Congfiguration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will configure the PatchTST model based on the `Economic_Data_1994_2025` dataset we processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Personal_Proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PatchTSTConfig, PatchTSTForPrediction, PatchTSTForPretraining\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For faster development\n",
    "device = torch.device('mps')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/model_data/Economic_Data_1994-2025.csv')\n",
    "dataset = dataset.drop(['DATE', 'Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding PatchTST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Context Length**\n",
    "\n",
    "    Context length is how far we look back in total. If we were trying to predict the closing price for the SP500 tomorrow, our context length would be how far we look back to make our prediction.\n",
    "\n",
    "- **Patch Length**\n",
    "\n",
    "    Patch length is like a subset of our context length. When looking at our entire context length, patch length is the looking at each individual week up until tomorrow to make our final prediction\n",
    "\n",
    "- **Patch Stride**\n",
    "\n",
    "    Patch stride is how far our patch length will move after observing an individual week. We can overlap weeks to see any comparisons.\n",
    "    \n",
    "For each batch we will pass N amount of rows. Each row has previous rows (context length) attached to it. For each row & it's context length we pass it into our model to train on. During the training process we will used. masked forecasting. This will mask the last portion of our patch's for the model to predict. It then check's it's guesses and updates its weights accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreTraining Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-training model will learn ***every*** column in our dataset from all dates. This will help the model develop relationships between all variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features we are including \n",
    "NUM_INPUT = len(dataset.columns)\n",
    "\n",
    "# Batch size for training\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# For What we are predicting\n",
    "NUM_TARGET = 2\n",
    "\n",
    "# How many steps we take in the context length\n",
    "CONTEXT_LEN = 120\n",
    "\n",
    "# How many steps we take in the context length\n",
    "PATCH_LEN = 20\n",
    "\n",
    "# How far we predict into the future:\n",
    "PRED_LEN = 10\n",
    "\n",
    "# How far we move our patch length\n",
    "PATCH_STRD = int(PATCH_LEN * .35)\n",
    "\n",
    "# How our model is trained\n",
    "MASK_TYPE = 'forecast'\n",
    "\n",
    "# Dimension of the model's internal representations\n",
    "D_MODEL = 256\n",
    "\n",
    "# Number of transformer encoder layers (should be multiple of d_model)\n",
    "# When increased should increase path_dropout.\n",
    "# default: 3\n",
    "NUM_HIDD_LAYERS = 4\n",
    "\n",
    "# Number of parallel attention mechanisms, allows model to focus on different temporal patterns simultaneously (4-8) ideal range\n",
    "# should divide d model evenly \n",
    "#\n",
    "# larger d_model and more attention heads benefit from higher dropout rates\n",
    "# default: 4\n",
    "NUM_ATT_HEAD = 8\n",
    "\n",
    "# Prevent overfitting by randomly deactivating components, acts as regularization during training\n",
    "ATT_DROP = .1\n",
    "\n",
    "# Applies dropout to the feed-forward networks within transformer blocks, \n",
    "FF_DROP = .2\n",
    "\n",
    "# Randomly skips entire layers or sub-pathways during training\n",
    "# rule of thumb (num_hidden_layers - 2) * .05\n",
    "PATH_DROP = (NUM_HIDD_LAYERS - 2) * .05\n",
    "\n",
    "# How many items are masked in our forcast\n",
    "NUM_PATCH = [int(BATCH_SIZE * .4)] # 20% of our batch\n",
    "\n",
    "# Configuring Model\n",
    "pretrain_config = PatchTSTConfig(\n",
    "    num_input_channels = NUM_INPUT,\n",
    "    context_length = CONTEXT_LEN,\n",
    "    patch_length = PATCH_LEN,\n",
    "    stride = PATCH_STRD,\n",
    "    mask_type='forecast',\n",
    "    num_forecast_mask_patches = NUM_PATCH,\n",
    "    do_mask_input=True,\n",
    "    prediction_length=PRED_LEN,\n",
    "    d_model=D_MODEL,\n",
    "    num_hidden_layers=NUM_HIDD_LAYERS,\n",
    "    num_attention_heads=NUM_ATT_HEAD,\n",
    "    attention_dropout=ATT_DROP,\n",
    "    ff_dropout=FF_DROP,\n",
    "    path_dropout=PATH_DROP,\n",
    ")\n",
    "\n",
    "pretrain_model = PatchTSTForPretraining(pretrain_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are splitting up our data into 2 portions. A `test` set and a `train` set. The test set is used to evalute our model based on training from the train set\n",
    "\n",
    "We split it 80/10/10, where 80% of our data is training data, and 10% of our data is testing, and 10% is validation for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up constraints for development\n",
    "num_train = int(len(dataset) * .7)\n",
    "num_test = int(len(dataset) * .2)\n",
    "num_val = int(len(dataset) * .1)\n",
    "\n",
    "# Breaking up the data into train/test sets.\n",
    "train = dataset[0: num_train]\n",
    "test = dataset[num_train:num_test + num_train]\n",
    "val = dataset[num_test+num_train:(num_test+num_train) + num_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion here grabs context windows for all rows in our train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a context window for each data point to feed into the model during training\n",
    "def create_sequence_windows(data, window_size):\n",
    "    windows = []\n",
    "    \n",
    "    # We start in the dataFrame at an index 'window_size' and look back depending on the window size\n",
    "    # We will grab a context window for all data points\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        windows.append(data.iloc[i:i+window_size].values)\n",
    "    return np.array(windows)\n",
    "\n",
    "train_windows = create_sequence_windows(train, CONTEXT_LEN)\n",
    "test_windows = create_sequence_windows(test, CONTEXT_LEN)\n",
    "val_windows = create_sequence_windows(val, CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts our data in PyTorch tensors for proper data types during training\n",
    "past_values_train = torch.tensor(train_windows, dtype=torch.float32)\n",
    "past_values_test = torch.tensor(test_windows, dtype=torch.float32)\n",
    "past_values_val = torch.tensor(val_windows, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing our data to be passed into our model for pre training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the tensors in a dataset for the dataloader to properly use\n",
    "data_train = TensorDataset(past_values_train)\n",
    "\n",
    "# Divides our data into batches based on the BATCH_SIZE\n",
    "dataloader_train = DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "data_test = TensorDataset(past_values_test)\n",
    "dataloader_test = DataLoader(data_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "data_val = TensorDataset(past_values_val)\n",
    "dataloader_val = DataLoader(data_val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = pretrain_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are training our model by doing the following:\n",
    "\n",
    "1. Divide Training into Epoch\n",
    "    - For each epoch we:\n",
    "        - Set model to train mode\n",
    "        - Pass in all of our data one batch size at a time\n",
    "        - After training on a patch we update our parameters\n",
    "        - Put our model into evaluation mode\n",
    "        - Test on our validation set and print results to output\n",
    "2. Final Train\n",
    "    - Once we pass through all epochs we do a final test on our\n",
    "        never seen data, `test` set. \n",
    "    - We iterate through our data one batch size at a time and evaluate the model\n",
    "        one last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/335 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 335/335 [04:27<00:00,  1.25it/s, loss=0.0769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.1480788578960433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 42/42 [00:08<00:00,  4.84it/s, loss=0.0879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 0 : 0.09393673789288316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 335/335 [04:25<00:00,  1.26it/s, loss=0.0491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.065046928689551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 42/42 [00:08<00:00,  4.93it/s, loss=0.0417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 1 : 0.04813251582284769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 335/335 [04:26<00:00,  1.26it/s, loss=0.0585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.045247424922105095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 42/42 [00:08<00:00,  4.94it/s, loss=0.0804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 2 : 0.07140913233160973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 335/335 [04:26<00:00,  1.26it/s, loss=0.0291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.038385463066732704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 42/42 [00:08<00:00,  4.70it/s, loss=0.0307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 3 : 0.02952000086328813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 335/335 [04:25<00:00,  1.26it/s, loss=0.029] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.03187815896721918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 42/42 [00:08<00:00,  4.96it/s, loss=0.04]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 4 : 0.040865891391322726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 335/335 [04:25<00:00,  1.26it/s, loss=0.0323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.03210745640099048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 42/42 [00:08<00:00,  4.67it/s, loss=0.0302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 5 : 0.03752407857349941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 335/335 [04:25<00:00,  1.26it/s, loss=0.0236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.03229936236765847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 42/42 [00:08<00:00,  4.80it/s, loss=0.054] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 6 : 0.050138216909198535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 335/335 [04:24<00:00,  1.27it/s, loss=0.0294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.028759574887356653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 42/42 [00:08<00:00,  4.97it/s, loss=0.0395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 7 : 0.036733215231271016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 335/335 [04:24<00:00,  1.27it/s, loss=0.0202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.027672830135075013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 42/42 [00:08<00:00,  4.95it/s, loss=0.0441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 8 : 0.04192496814011108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 335/335 [04:24<00:00,  1.27it/s, loss=0.0288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.028575669023305623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 42/42 [00:08<00:00,  4.93it/s, loss=0.031] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 9 : 0.033252003602683544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 91/91 [00:18<00:00,  4.82it/s, loss=0.0205] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for test set : 0.031185404172392336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(pretrain_model.parameters(), lr=.001)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Allows for progress bar during training per epoch\n",
    "    loop = tqdm(dataloader_train, leave=True)\n",
    "    losses = []\n",
    "    \n",
    "    for batch in loop:\n",
    "        \n",
    "        # Puts model in train mode\n",
    "        pretrain_model.train()\n",
    "        \n",
    "        # Clears any previous gradient calculations\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Transfers batch onto GPU for faster processing\n",
    "        past_values = batch[0].to(device)\n",
    "        \n",
    "        # Foward pass through our model, generates predictions\n",
    "        outputs = pretrain_model(past_values=past_values)\n",
    "        \n",
    "        # Get's the loss for our predictions (how far off our predictions were)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Calculates which weights contributed to the error of our prediction\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(pretrain_model.parameters(), max_norm=3.0)\n",
    "        \n",
    "        # Updates the optimizer based on the calculations made from loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(\"Mean Training Loss\", np.mean(losses))\n",
    "        \n",
    "    pretrain_model.eval()\n",
    "    losses = []\n",
    "\n",
    "    loop = tqdm(dataloader_val, leave=True)\n",
    "    \n",
    "    for batch in loop:\n",
    "        pretrain_model.eval()\n",
    "        \n",
    "        past_values = batch[0].to(device)\n",
    "        \n",
    "        outputs = pretrain_model(past_values=past_values)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(f\"Mean Training Loss for validation set on EPOCH {epoch} : {np.mean(losses)}\")\n",
    "\n",
    "pretrain_model.eval()\n",
    "losses = []\n",
    "\n",
    "loop = tqdm(dataloader_test, leave=True)\n",
    "\n",
    "for batch in loop:\n",
    "    \n",
    "    past_values = batch[0].to(device)\n",
    "    \n",
    "    outputs = pretrain_model(past_values=past_values)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    \n",
    "    loop.set_description(f'Test')\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "print(f\"Mean Training Loss for test set : {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pretrain_model.state_dict(), 'pt_m_2ynorm_v1.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters Set 1 (Not Good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT = len(dataset.columns)\n",
    "BATCH_SIZE = 16\n",
    "CONTEXT_LEN = 190\n",
    "PATCH_LEN = 10\n",
    "PATCH_STRD = 8\n",
    "MASK_TYPE = 'forecast'\n",
    "NUM_PATCH = [int(BATCH_SIZE * .2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pretrained_model_no_mask** \n",
    "\n",
    "    Mean Training Loss for test set : 0.0191\n",
    "\n",
    "- **pretrained_model_mask** \n",
    "\n",
    "    Mean Training Loss for test set : 0.0199\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters Set 2 (Not Good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT = len(dataset.columns)\n",
    "BATCH_SIZE = 16\n",
    "CONTEXT_LEN = 60 # Only looking back 60 days instead of 190\n",
    "PATCH_LEN = 5 # Look at each prior week before for each data point in context length \n",
    "PATCH_STRD = 4 # Move window while keeping the last week's most recent day in set.\n",
    "MASK_TYPE = 'forecast'\n",
    "NUM_PATCH = [int(BATCH_SIZE * .2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pretrained_model_no_mask** \n",
    "\n",
    "    Mean Training Loss for test set : 80.5\n",
    "\n",
    "- **pretrained_model_mask** \n",
    "\n",
    "    Mean Training Loss for test set : 1.29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Set 3 (Past Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT = len(dataset.columns)\n",
    "BATCH_SIZE = 32 # increase batch\n",
    "CONTEXT_LEN = 90 # Middle Ground context length\n",
    "PATCH_LEN = 10 # 2 week patch\n",
    "PATCH_STRD = 5 # 1 week movement\n",
    "MASK_TYPE = 'forecast'\n",
    "NUM_PATCH = [int(BATCH_SIZE * .4)] # 40% of our batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pretrained_model_mask** \n",
    "\n",
    "    Mean Training Loss for test set: `.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Set 4 (Current Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT = len(dataset.columns)\n",
    "BATCH_SIZE = 64 # increase batch\n",
    "CONTEXT_LEN = 120 # Changed\n",
    "PATCH_LEN = 20 # 4 week patch\n",
    "PATCH_STRD = 10 # 2 week movement\n",
    "MASK_TYPE = 'forecast'\n",
    "NUM_PATCH = [int(BATCH_SIZE * .4)] # 40% of our batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pretrained_model_mask** \n",
    "\n",
    "    Mean Training Loss for test set: `.14`\n",
    "\n",
    "    Training Output:\n",
    "    \n",
    "    ```\n",
    "        Epoch 0: 100%|██████████| 96/96 [00:21<00:00,  4.47it/s, loss=0.249]\n",
    "        Mean Training Loss 0.3704576228434841\n",
    "        Epoch 0: 100%|██████████| 11/11 [00:00<00:00, 12.22it/s, loss=0.174]\n",
    "        Mean Training Loss for validation set on EPOCH 0 : 0.259783918207342\n",
    "        Epoch 1: 100%|██████████| 96/96 [00:21<00:00,  4.45it/s, loss=0.193]\n",
    "        Mean Training Loss 0.21437909283364812\n",
    "        Epoch 1: 100%|██████████| 11/11 [00:00<00:00, 12.56it/s, loss=0.134]\n",
    "        Mean Training Loss for validation set on EPOCH 1 : 0.188843966207721\n",
    "        Epoch 2: 100%|██████████| 96/96 [00:20<00:00,  4.62it/s, loss=0.167]\n",
    "        Mean Training Loss 0.18412007112056017\n",
    "        Epoch 2: 100%|██████████| 11/11 [00:00<00:00, 12.41it/s, loss=0.0882]\n",
    "        Mean Training Loss for validation set on EPOCH 2 : 0.1636897704818032\n",
    "        Epoch 3: 100%|██████████| 96/96 [00:21<00:00,  4.52it/s, loss=0.196]\n",
    "        Mean Training Loss 0.1677302731356273\n",
    "        Epoch 3: 100%|██████████| 11/11 [00:00<00:00, 11.90it/s, loss=0.0843]\n",
    "        Mean Training Loss for validation set on EPOCH 3 : 0.16888367046009412\n",
    "        Epoch 4: 100%|██████████| 96/96 [00:20<00:00,  4.63it/s, loss=0.155]\n",
    "        Mean Training Loss 0.15664897416718304\n",
    "        Epoch 4: 100%|██████████| 11/11 [00:00<00:00, 12.23it/s, loss=0.107]\n",
    "        Mean Training Loss for validation set on EPOCH 4 : 0.15169502049684525\n",
    "        Epoch 5: 100%|██████████| 96/96 [00:19<00:00,  4.82it/s, loss=0.142]\n",
    "        Mean Training Loss 0.15216751280240715\n",
    "        Epoch 5: 100%|██████████| 11/11 [00:00<00:00, 13.08it/s, loss=0.0657]\n",
    "        Mean Training Loss for validation set on EPOCH 5 : 0.14568436348980124\n",
    "        Epoch 6: 100%|██████████| 96/96 [00:19<00:00,  5.00it/s, loss=0.158]\n",
    "        Mean Training Loss 0.14830683163988093\n",
    "        Epoch 6: 100%|██████████| 11/11 [00:00<00:00, 12.93it/s, loss=0.0564]\n",
    "        Mean Training Loss for validation set on EPOCH 6 : 0.1488013728098436\n",
    "        Epoch 7: 100%|██████████| 96/96 [00:19<00:00,  4.94it/s, loss=0.159]\n",
    "        Mean Training Loss 0.14496231932813922\n",
    "        Epoch 7: 100%|██████████| 11/11 [00:00<00:00, 12.66it/s, loss=0.0639]\n",
    "        Mean Training Loss for validation set on EPOCH 7 : 0.16032350334254178\n",
    "        Epoch 8: 100%|██████████| 96/96 [00:19<00:00,  5.01it/s, loss=0.151]\n",
    "        Mean Training Loss 0.14232469288011393\n",
    "        Epoch 8: 100%|██████████| 11/11 [00:00<00:00, 13.03it/s, loss=0.0615]\n",
    "        Mean Training Loss for validation set on EPOCH 8 : 0.14566036821766334\n",
    "        Epoch 9: 100%|██████████| 96/96 [00:20<00:00,  4.79it/s, loss=0.148]\n",
    "        Mean Training Loss 0.13476501870900393\n",
    "        Epoch 9: 100%|██████████| 11/11 [00:00<00:00, 12.03it/s, loss=0.0496]\n",
    "        Mean Training Loss for validation set on EPOCH 9 : 0.14310653473843227\n",
    "        Test: 100%|██████████| 11/11 [00:00<00:00, 13.17it/s, loss=0.132]\n",
    "        Mean Training Loss for test set : 0.14791631867939775\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Set 4 (In Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Set used 2 year rolling window for normalization\n",
    "\n",
    "\n",
    "NUM_INPUT = len(dataset.columns)\n",
    "BATCH_SIZE = 16\n",
    "NUM_TARGET = 2\n",
    "CONTEXT_LEN = 120\n",
    "PATCH_LEN = 20\n",
    "PRED_LEN = 10\n",
    "PATCH_STRD = int(PATCH_LEN * .35)\n",
    "MASK_TYPE = 'forecast'\n",
    "D_MODEL = 256\n",
    "NUM_HIDD_LAYERS = 4\n",
    "NUM_ATT_HEAD = 8\n",
    "ATT_DROP = .1\n",
    "FF_DROP = .2\n",
    "PATH_DROP = (NUM_HIDD_LAYERS - 2) * .05\n",
    "NUM_PATCH = [int(BATCH_SIZE * .4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pt_m_2ynorm_v1**\n",
    "```\n",
    "        0%|          | 0/335 [00:00<?, ?it/s]\n",
    "        Epoch 0: 100%|██████████| 335/335 [04:27<00:00,  1.25it/s, loss=0.0769]\n",
    "        Mean Training Loss 0.1480788578960433\n",
    "        Epoch 0: 100%|██████████| 42/42 [00:08<00:00,  4.84it/s, loss=0.0879]\n",
    "        Mean Training Loss for validation set on EPOCH 0 : 0.09393673789288316\n",
    "        Epoch 1: 100%|██████████| 335/335 [04:25<00:00,  1.26it/s, loss=0.0491]\n",
    "        Mean Training Loss 0.065046928689551\n",
    "        Epoch 1: 100%|██████████| 42/42 [00:08<00:00,  4.93it/s, loss=0.0417]\n",
    "        Mean Training Loss for validation set on EPOCH 1 : 0.04813251582284769\n",
    "        Epoch 2: 100%|██████████| 335/335 [04:26<00:00,  1.26it/s, loss=0.0585]\n",
    "        Mean Training Loss 0.045247424922105095\n",
    "        Epoch 2: 100%|██████████| 42/42 [00:08<00:00,  4.94it/s, loss=0.0804]\n",
    "        Mean Training Loss for validation set on EPOCH 2 : 0.07140913233160973\n",
    "        Epoch 3: 100%|██████████| 335/335 [04:26<00:00,  1.26it/s, loss=0.0291]\n",
    "        Mean Training Loss 0.038385463066732704\n",
    "        Epoch 3: 100%|██████████| 42/42 [00:08<00:00,  4.70it/s, loss=0.0307]\n",
    "        Mean Training Loss for validation set on EPOCH 3 : 0.02952000086328813\n",
    "        Epoch 4: 100%|██████████| 335/335 [04:25<00:00,  1.26it/s, loss=0.029] \n",
    "        Mean Training Loss 0.03187815896721918\n",
    "        Epoch 4: 100%|██████████| 42/42 [00:08<00:00,  4.96it/s, loss=0.04]  \n",
    "        Mean Training Loss for validation set on EPOCH 4 : 0.040865891391322726\n",
    "        Epoch 5: 100%|██████████| 335/335 [04:25<00:00,  1.26it/s, loss=0.0323]\n",
    "        Mean Training Loss 0.03210745640099048\n",
    "        Epoch 5: 100%|██████████| 42/42 [00:08<00:00,  4.67it/s, loss=0.0302]\n",
    "        Mean Training Loss for validation set on EPOCH 5 : 0.03752407857349941\n",
    "        Epoch 6: 100%|██████████| 335/335 [04:25<00:00,  1.26it/s, loss=0.0236]\n",
    "        Mean Training Loss 0.03229936236765847\n",
    "        Epoch 6: 100%|██████████| 42/42 [00:08<00:00,  4.80it/s, loss=0.054] \n",
    "        Mean Training Loss for validation set on EPOCH 6 : 0.050138216909198535\n",
    "        Epoch 7: 100%|██████████| 335/335 [04:24<00:00,  1.27it/s, loss=0.0294]\n",
    "        Mean Training Loss 0.028759574887356653\n",
    "        Epoch 7: 100%|██████████| 42/42 [00:08<00:00,  4.97it/s, loss=0.0395]\n",
    "        Mean Training Loss for validation set on EPOCH 7 : 0.036733215231271016\n",
    "        Epoch 8: 100%|██████████| 335/335 [04:24<00:00,  1.27it/s, loss=0.0202]\n",
    "        Mean Training Loss 0.027672830135075013\n",
    "        Epoch 8: 100%|██████████| 42/42 [00:08<00:00,  4.95it/s, loss=0.0441]\n",
    "        Mean Training Loss for validation set on EPOCH 8 : 0.04192496814011108\n",
    "        Epoch 9: 100%|██████████| 335/335 [04:24<00:00,  1.27it/s, loss=0.0288]\n",
    "        Mean Training Loss 0.028575669023305623\n",
    "        Epoch 9: 100%|██████████| 42/42 [00:08<00:00,  4.93it/s, loss=0.031] \n",
    "        Mean Training Loss for validation set on EPOCH 9 : 0.033252003602683544\n",
    "        Test: 100%|██████████| 91/91 [00:18<00:00,  4.82it/s, loss=0.0205] \n",
    "        Mean Training Loss for test set : 0.031185404172392336\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many features we are including \n",
    "NUM_INPUT = 2\n",
    "\n",
    "# Batch size for training\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# For What we are predicting\n",
    "NUM_TARGET = 2\n",
    "\n",
    "# How many steps we take in the context length\n",
    "CONTEXT_LEN = 120\n",
    "\n",
    "# How many steps we take in the context length\n",
    "PATCH_LEN = 20\n",
    "\n",
    "# How far we move our patch length\n",
    "PATCH_STRD = 10\n",
    "\n",
    "NUM_INPUT = len(dataset.columns)\n",
    "NUM_TARGET = 2\n",
    "CONTEXT_LEN = 120\n",
    "PATCH_LEN = 20\n",
    "PRED_LEN = 10\n",
    "PATCH_STRD = int(PATCH_LEN * .35)\n",
    "MASK_TYPE = 'forecast'\n",
    "D_MODEL = 256\n",
    "NUM_HIDD_LAYERS = 4\n",
    "NUM_ATT_HEAD = 8\n",
    "ATT_DROP = .1\n",
    "FF_DROP = .2\n",
    "PATH_DROP = (NUM_HIDD_LAYERS - 2) * .05\n",
    "NUM_PATCH = [int(BATCH_SIZE * .4)]\n",
    "\n",
    "# How many days to predict into the future\n",
    "PRED_LEN = 10 # One Month Prediction\n",
    "\n",
    "ft_config = PatchTSTConfig(\n",
    "    num_input_channels = NUM_INPUT,\n",
    "    num_targets = NUM_TARGET,\n",
    "    context_length = CONTEXT_LEN,\n",
    "    patch_length = PATCH_LEN,\n",
    "    stride = PATCH_STRD,\n",
    "    prediction_length=PRED_LEN,\n",
    "    d_model=D_MODEL,\n",
    "    num_hidden_layers=NUM_HIDD_LAYERS,\n",
    "    num_attention_heads=NUM_ATT_HEAD,\n",
    "    attention_dropout=ATT_DROP,\n",
    "    ff_dropout=FF_DROP,\n",
    "    path_dropout=PATH_DROP,\n",
    ")\n",
    "\n",
    "features_to_pred = ['^GSPC Close', '^DJI Close']\n",
    "\n",
    "ft_model = PatchTSTForPrediction(ft_config)\n",
    "\n",
    "# First, load your saved pretrained model\n",
    "pretrained_weights = torch.load('./pt_m_2ynorm_v1.bin')\n",
    "\n",
    "# Copy weights from the encoder part of the pretrained model\n",
    "# This will transfer only the compatible weights\n",
    "prediction_model_dict = ft_model.state_dict()\n",
    "\n",
    "for name, param in pretrained_weights.items():\n",
    "    if 'encoder' in name:\n",
    "        # The encoder part is usually named like 'encoder.xxx' in both models\n",
    "        if name in prediction_model_dict:\n",
    "            prediction_model_dict[name] = param\n",
    "ft_model.load_state_dict(prediction_model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up constraints for development\n",
    "num_train = int(len(dataset) * .7)\n",
    "num_test = int(len(dataset) * .2)\n",
    "num_val = int(len(dataset) * .1)\n",
    "\n",
    "dataset = dataset[features_to_pred]\n",
    "targ_data = dataset\n",
    "\n",
    "train_targ = targ_data[0: num_train]\n",
    "train_feat = dataset[0: num_train]\n",
    "\n",
    "test_targ = targ_data[num_train:num_test + num_train]\n",
    "test_feat = dataset[num_train:num_test + num_train]\n",
    "\n",
    "val_targ = targ_data[num_test+num_train:(num_test+num_train) + num_val]\n",
    "val_feat = dataset[num_test+num_train:(num_test+num_train) + num_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Target/Input Features**\n",
    "\n",
    "This part is a little odd. \n",
    "\n",
    "- **Input Features**\n",
    "\n",
    "    To get the target features all we need to do is construct a window that looks at the past N amount of days for each data point.\n",
    "    We include the features we want to predict which makes it **Self Supervised**. \n",
    "\n",
    "- **Output Features**\n",
    "\n",
    "    What we are doing is getting the actual targets we want to predict and making a future window for just the 2 features. \n",
    "    In this case we are looking 90 days into the future, or what the model is predicting, and grabbing those values. This is used \n",
    "    for the model to evalute it's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a context window for each data point to feed into the model during training\n",
    "def create_sequence_windows(data, window_size):\n",
    "    windows = []\n",
    "    \n",
    "    # We start in the dataFrame at an index 'window_size' and look back depending on the window size\n",
    "    # We will grab a context window for all data points\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        windows.append(data.iloc[i:i+window_size].values)\n",
    "    return np.array(windows)\n",
    "\n",
    "train_feat_windows = create_sequence_windows(train_feat, CONTEXT_LEN)\n",
    "test_feat_windows = create_sequence_windows(test_feat, CONTEXT_LEN)\n",
    "val_feat_windows = create_sequence_windows(val_feat, CONTEXT_LEN)\n",
    "\n",
    "# Remove values at the end that don't have enough future data\n",
    "train_feat_windows = train_feat_windows[0:len(train_feat_windows) - PRED_LEN]\n",
    "test_feat_windows = test_feat_windows[0:len(test_feat_windows) - PRED_LEN]\n",
    "val_feat_windows = val_feat_windows[0:len(val_feat_windows) - PRED_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets indices for the target variables, starting from where we first start predicting with a full context length\n",
    "# to the last index that will allow for a full prediction\n",
    "train_targ_indices = range(CONTEXT_LEN, len(train_feat) - PRED_LEN + 1)\n",
    "test_targ_indices = range(CONTEXT_LEN, len(test_feat) - PRED_LEN + 1)\n",
    "val_targ_indices = range(CONTEXT_LEN, len(val_feat) - PRED_LEN + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targ_windows = [train_targ.iloc[i:i+PRED_LEN].values for i in train_targ_indices]\n",
    "test_targ_windows = [test_targ.iloc[i:i+PRED_LEN].values for i in test_targ_indices]\n",
    "val_targ_windows = [val_targ.iloc[i:i+PRED_LEN].values for i in val_targ_indices]\n",
    "\n",
    "train_targ_windows = np.array(train_targ_windows)\n",
    "test_targ_windows = np.array(test_targ_windows )\n",
    "val_targ_windows = np.array(val_targ_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1436, 120, 2]), torch.Size([1436, 10, 2]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "past_train = torch.tensor(train_feat_windows, dtype=torch.float32)\n",
    "past_test = torch.tensor(test_feat_windows, dtype=torch.float32)\n",
    "past_val = torch.tensor(val_feat_windows, dtype=torch.float32)\n",
    "\n",
    "future_train = torch.tensor(train_targ_windows, dtype=torch.float32)\n",
    "future_test = torch.tensor(test_targ_windows, dtype=torch.float32)\n",
    "future_val = torch.tensor(val_targ_windows, dtype=torch.float32)\n",
    "past_test.shape, future_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(past_train, future_train)\n",
    "test_data = TensorDataset(past_test, future_test)\n",
    "val_data = TensorDataset(past_val, future_val)\n",
    "\n",
    "dataloader_train = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_test = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_val = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "ft_model = ft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 335/335 [00:19<00:00, 17.16it/s, loss=0.0678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.08337097368133602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 41/41 [00:00<00:00, 55.91it/s, loss=0.752] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 0 : 0.08151997666715122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 335/335 [00:17<00:00, 18.96it/s, loss=0.0318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.06233765845979328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 41/41 [00:00<00:00, 72.46it/s, loss=nan]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 1 : 0.06248722393065691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 335/335 [00:17<00:00, 19.05it/s, loss=0.0433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.05805389129959825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 41/41 [00:00<00:00, 74.28it/s, loss=0.0472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 2 : 0.048045138896601954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 335/335 [00:17<00:00, 19.18it/s, loss=0.0148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.051378124968996686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 41/41 [00:00<00:00, 73.34it/s, loss=0.0385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 3 : 0.048487595759513905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 335/335 [00:17<00:00, 19.16it/s, loss=0.083] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.04694074744275257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 41/41 [00:00<00:00, 74.89it/s, loss=0.0354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 4 : 0.04428161417202252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 335/335 [00:17<00:00, 19.09it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.0436276269437217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 41/41 [00:00<00:00, 73.32it/s, loss=0.0387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 5 : 0.041960370749598595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 335/335 [00:17<00:00, 19.09it/s, loss=0.0299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.04130253072883656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 41/41 [00:00<00:00, 73.08it/s, loss=0.0394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 6 : 0.04476174085241992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 335/335 [00:17<00:00, 19.15it/s, loss=0.0076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.03777332752665032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 41/41 [00:00<00:00, 73.14it/s, loss=0.0511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 7 : 0.046855011591460646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 335/335 [00:17<00:00, 18.98it/s, loss=0.0176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.03557930594282364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 41/41 [00:00<00:00, 73.56it/s, loss=nan]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 8 : 0.048434071335941555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 335/335 [00:17<00:00, 18.86it/s, loss=0.0375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss 0.033892850247003246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 41/41 [00:00<00:00, 73.40it/s, loss=0.0291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for validation set on EPOCH 9 : 0.04664606033120214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 90/90 [00:01<00:00, 66.85it/s, loss=0.606] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Loss for test set : 0.15808176810152075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(ft_model.parameters(), lr=.0001)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Allows for progress bar during training per epoch\n",
    "    loop = tqdm(dataloader_train, leave=True)\n",
    "    losses = []\n",
    "    \n",
    "    for past_values, future_values in loop:\n",
    "        ft_model.train()\n",
    "        \n",
    "        # Clears any previous gradient calculations\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Transfers batch onto GPU for faster processing\n",
    "        past_values = past_values.to(device)\n",
    "        future_values = future_values.to(device)\n",
    "        \n",
    "        # Foward pass through our model, generates predictions\n",
    "        outputs = ft_model(past_values=past_values, future_values=future_values)\n",
    "        \n",
    "        # Get's the loss for our predictions (how far off our predictions were)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Calculates which weights contributed to the error of our prediction\n",
    "        loss.backward()\n",
    "        \n",
    "        # Add gradient clipping to scale down calculated gradient\n",
    "        torch.nn.utils.clip_grad_norm_(ft_model.parameters(), max_norm=1)\n",
    "        \n",
    "        # Updates the optimizer based on the calculations made from loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(\"Mean Training Loss\", np.nanmean(losses))\n",
    "        \n",
    "    ft_model.eval()\n",
    "    losses = []\n",
    "\n",
    "    loop = tqdm(dataloader_val, leave=True)\n",
    "    \n",
    "    for past_values, future_values in loop:\n",
    "        \n",
    "        past_values = past_values.to(device)\n",
    "        future_values = future_values.to(device)\n",
    "        \n",
    "        # Foward pass through our model, generates predictions\n",
    "        outputs = ft_model(past_values=past_values, future_values=future_values)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(f\"Mean Training Loss for validation set on EPOCH {epoch} : {np.nanmean(losses)}\")\n",
    "\n",
    "ft_model.eval()\n",
    "losses = []\n",
    "\n",
    "loop = tqdm(dataloader_test, leave=True)\n",
    "\n",
    "for past_values, future_values in loop:\n",
    "    \n",
    "    past_values = past_values.to(device)\n",
    "    future_values = future_values.to(device)\n",
    "    \n",
    "    # Foward pass through our model, generates predictions\n",
    "    outputs = ft_model(past_values=past_values, future_values=future_values)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    \n",
    "    loop.set_description(f'Test')\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "print(f\"Mean Training Loss for test set : {np.nanmean(losses)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **pt_v2_ft_v1 model** (3 Day Target)\n",
    " \n",
    " ```\n",
    "      Epoch 0: 100%|██████████| 670/670 [00:25<00:00, 26.71it/s, loss=0.0121] \n",
    "      Mean Training Loss 0.05789172873908737\n",
    "      Epoch 0: 100%|██████████| 83/83 [00:00<00:00, 99.51it/s, loss=0.0248]  \n",
    "      Mean Training Loss for validation set on EPOCH 0 : 0.01956911194587059\n",
    "      Epoch 1: 100%|██████████| 670/670 [00:22<00:00, 29.15it/s, loss=0.00422]\n",
    "      Mean Training Loss 0.02624725556737785\n",
    "      Epoch 1: 100%|██████████| 83/83 [00:00<00:00, 93.56it/s, loss=0.0315]  \n",
    "      Mean Training Loss for validation set on EPOCH 1 : 0.015010590838964086\n",
    "      Epoch 2: 100%|██████████| 670/670 [00:23<00:00, 28.99it/s, loss=0.00633]\n",
    "      Mean Training Loss 0.025074393672993713\n",
    "      Epoch 2: 100%|██████████| 83/83 [00:00<00:00, 120.43it/s, loss=0.0235] \n",
    "      Mean Training Loss for validation set on EPOCH 2 : 0.01816328740061586\n",
    "      Epoch 3: 100%|██████████| 670/670 [00:23<00:00, 28.68it/s, loss=0.0121] \n",
    "      Mean Training Loss 0.023857501743764463\n",
    "      Epoch 3: 100%|██████████| 83/83 [00:00<00:00, 111.34it/s, loss=0.0134] \n",
    "      Mean Training Loss for validation set on EPOCH 3 : 0.01569510066141206\n",
    "      Epoch 4: 100%|██████████| 670/670 [00:23<00:00, 28.43it/s, loss=0.0102] \n",
    "      Mean Training Loss 0.022569548738633854\n",
    "      Epoch 4: 100%|██████████| 83/83 [00:00<00:00, 90.46it/s, loss=0.0167]  \n",
    "      Mean Training Loss for validation set on EPOCH 4 : 0.015427874883704156\n",
    "      Epoch 5: 100%|██████████| 670/670 [00:23<00:00, 28.66it/s, loss=0.0774] \n",
    "      Mean Training Loss 0.022100595781690817\n",
    "      Epoch 5: 100%|██████████| 83/83 [00:00<00:00, 88.42it/s, loss=0.0106] \n",
    "      Mean Training Loss for validation set on EPOCH 5 : 0.015575622017663646\n",
    "      Epoch 6: 100%|██████████| 670/670 [00:23<00:00, 28.21it/s, loss=0.00743]\n",
    "      Mean Training Loss 0.022202242850395505\n",
    "      Epoch 6: 100%|██████████| 83/83 [00:00<00:00, 96.81it/s, loss=0.0115]  \n",
    "      Mean Training Loss for validation set on EPOCH 6 : 0.015273614887552089\n",
    "      Epoch 7: 100%|██████████| 670/670 [00:25<00:00, 26.68it/s, loss=0.0191] \n",
    "      Mean Training Loss 0.021336314458149805\n",
    "      Epoch 7: 100%|██████████| 83/83 [00:00<00:00, 94.89it/s, loss=0.0196] \n",
    "      Mean Training Loss for validation set on EPOCH 7 : 0.016381341576890415\n",
    "      Epoch 8: 100%|██████████| 670/670 [00:24<00:00, 27.09it/s, loss=0.0165] \n",
    "      Mean Training Loss 0.02155865324146823\n",
    "      Epoch 8: 100%|██████████| 83/83 [00:00<00:00, 112.52it/s, loss=0.00985]\n",
    "      Mean Training Loss for validation set on EPOCH 8 : 0.0158606843843338\n",
    "      Epoch 9: 100%|██████████| 670/670 [00:23<00:00, 28.26it/s, loss=0.00341]\n",
    "      Mean Training Loss 0.021019455494963802\n",
    "      Epoch 9: 100%|██████████| 83/83 [00:00<00:00, 104.47it/s, loss=0.00861]\n",
    "      Mean Training Loss for validation set on EPOCH 9 : 0.015973350246358348\n",
    "      Test: 100%|██████████| 181/181 [00:01<00:00, 119.31it/s, loss=0.00236]\n",
    "      Mean Training Loss for test set : 0.052744830301216704\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pt_m_2ynorm_v1_ft_v1_model** (7 Day) (16 Batch)\n",
    "\n",
    "    ```\n",
    "    Epoch 0: 100%|██████████| 335/335 [00:20<00:00, 16.22it/s, loss=0.301] \n",
    "    Mean Training Loss 0.073840651099584\n",
    "    Epoch 0: 100%|██████████| 41/41 [00:00<00:00, 71.74it/s, loss=0.0432]\n",
    "    Mean Training Loss for validation set on EPOCH 0 : 0.03777422633294652\n",
    "    Epoch 1: 100%|██████████| 335/335 [00:17<00:00, 19.41it/s, loss=0.0714]\n",
    "    Mean Training Loss 0.04888085387091139\n",
    "    Epoch 1: 100%|██████████| 41/41 [00:00<00:00, 72.85it/s, loss=0.0401]\n",
    "    Mean Training Loss for validation set on EPOCH 1 : 0.037776948066382876\n",
    "    Epoch 2: 100%|██████████| 335/335 [00:17<00:00, 19.37it/s, loss=0.0283]\n",
    "    Mean Training Loss 0.04241640451850731\n",
    "    Epoch 2: 100%|██████████| 41/41 [00:00<00:00, 73.18it/s, loss=0.0363]\n",
    "    Mean Training Loss for validation set on EPOCH 2 : 0.03229101202109965\n",
    "    Epoch 3: 100%|██████████| 335/335 [00:17<00:00, 19.14it/s, loss=0.0151]\n",
    "    Mean Training Loss 0.038641006752515014\n",
    "    Epoch 3: 100%|██████████| 41/41 [00:00<00:00, 72.04it/s, loss=0.0368]\n",
    "    Mean Training Loss for validation set on EPOCH 3 : 0.03264551564324193\n",
    "    Epoch 4: 100%|██████████| 335/335 [00:17<00:00, 19.33it/s, loss=0.0525]\n",
    "    Mean Training Loss 0.0364013954969262\n",
    "    Epoch 4: 100%|██████████| 41/41 [00:00<00:00, 72.63it/s, loss=0.0521]\n",
    "    Mean Training Loss for validation set on EPOCH 4 : 0.035987246418144644\n",
    "    Epoch 5: 100%|██████████| 335/335 [00:17<00:00, 19.34it/s, loss=0.0207]\n",
    "    Mean Training Loss 0.03424341900746769\n",
    "    Epoch 5: 100%|██████████| 41/41 [00:00<00:00, 72.89it/s, loss=0.0183]\n",
    "    Mean Training Loss for validation set on EPOCH 5 : 0.03403225018665558\n",
    "    Epoch 6: 100%|██████████| 335/335 [00:17<00:00, 19.39it/s, loss=0.0163]\n",
    "    Mean Training Loss 0.03328820165079921\n",
    "    Epoch 6: 100%|██████████| 41/41 [00:00<00:00, 71.66it/s, loss=0.0434]\n",
    "    Mean Training Loss for validation set on EPOCH 6 : 0.030725138897939427\n",
    "    Epoch 7: 100%|██████████| 335/335 [00:17<00:00, 19.32it/s, loss=0.0216]\n",
    "    Mean Training Loss 0.03110665796574817\n",
    "    Epoch 7: 100%|██████████| 41/41 [00:00<00:00, 73.13it/s, loss=0.0348]\n",
    "    Mean Training Loss for validation set on EPOCH 7 : 0.03414890083779649\n",
    "    Epoch 8: 100%|██████████| 335/335 [00:17<00:00, 19.46it/s, loss=0.051] \n",
    "    Mean Training Loss 0.03023807040568608\n",
    "    Epoch 8: 100%|██████████| 41/41 [00:00<00:00, 72.56it/s, loss=0.0396]\n",
    "    Mean Training Loss for validation set on EPOCH 8 : 0.03446808239308799\n",
    "    Epoch 9: 100%|██████████| 335/335 [00:17<00:00, 19.38it/s, loss=0.0318]\n",
    "    Mean Training Loss 0.027910547305954925\n",
    "    Epoch 9: 100%|██████████| 41/41 [00:00<00:00, 73.05it/s, loss=0.037] \n",
    "    Mean Training Loss for validation set on EPOCH 9 : 0.037568732596388675\n",
    "    Test: 100%|██████████| 90/90 [00:01<00:00, 65.61it/s, loss=0.155] \n",
    "    Mean Training Loss for test set : 0.10987232279860311\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pt_m_2ynorm_v1_ft_v1_model** (2 Day) (16 Batch)(lr = .0001)(max_norm=1)\n",
    "```\n",
    "    Mean Loss for test set : .0447\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pt_m_2ynorm_v1_ft_v1_model**(10 Day)(16 Batch)(lr = .0001)(max_norm=1)\n",
    "```\n",
    "Epoch 0: 100%|██████████| 335/335 [00:19<00:00, 17.16it/s, loss=0.0678]\n",
    "Mean Training Loss 0.08337097368133602\n",
    "Epoch 0: 100%|██████████| 41/41 [00:00<00:00, 55.91it/s, loss=0.752] \n",
    "Mean Training Loss for validation set on EPOCH 0 : 0.08151997666715122\n",
    "Epoch 1: 100%|██████████| 335/335 [00:17<00:00, 18.96it/s, loss=0.0318]\n",
    "Mean Training Loss 0.06233765845979328\n",
    "Epoch 1: 100%|██████████| 41/41 [00:00<00:00, 72.46it/s, loss=nan]   \n",
    "Mean Training Loss for validation set on EPOCH 1 : 0.06248722393065691\n",
    "Epoch 2: 100%|██████████| 335/335 [00:17<00:00, 19.05it/s, loss=0.0433]\n",
    "Mean Training Loss 0.05805389129959825\n",
    "Epoch 2: 100%|██████████| 41/41 [00:00<00:00, 74.28it/s, loss=0.0472]\n",
    "Mean Training Loss for validation set on EPOCH 2 : 0.048045138896601954\n",
    "Epoch 3: 100%|██████████| 335/335 [00:17<00:00, 19.18it/s, loss=0.0148]\n",
    "Mean Training Loss 0.051378124968996686\n",
    "Epoch 3: 100%|██████████| 41/41 [00:00<00:00, 73.34it/s, loss=0.0385]\n",
    "Mean Training Loss for validation set on EPOCH 3 : 0.048487595759513905\n",
    "Epoch 4: 100%|██████████| 335/335 [00:17<00:00, 19.16it/s, loss=0.083] \n",
    "Mean Training Loss 0.04694074744275257\n",
    "Epoch 4: 100%|██████████| 41/41 [00:00<00:00, 74.89it/s, loss=0.0354]\n",
    "Mean Training Loss for validation set on EPOCH 4 : 0.04428161417202252\n",
    "Epoch 5: 100%|██████████| 335/335 [00:17<00:00, 19.09it/s, loss=0.157] \n",
    "Mean Training Loss 0.0436276269437217\n",
    "Epoch 5: 100%|██████████| 41/41 [00:00<00:00, 73.32it/s, loss=0.0387]\n",
    "Mean Training Loss for validation set on EPOCH 5 : 0.041960370749598595\n",
    "Epoch 6: 100%|██████████| 335/335 [00:17<00:00, 19.09it/s, loss=0.0299]\n",
    "Mean Training Loss 0.04130253072883656\n",
    "Epoch 6: 100%|██████████| 41/41 [00:00<00:00, 73.08it/s, loss=0.0394]\n",
    "Mean Training Loss for validation set on EPOCH 6 : 0.04476174085241992\n",
    "Epoch 7: 100%|██████████| 335/335 [00:17<00:00, 19.15it/s, loss=0.0076]\n",
    "Mean Training Loss 0.03777332752665032\n",
    "Epoch 7: 100%|██████████| 41/41 [00:00<00:00, 73.14it/s, loss=0.0511]\n",
    "Mean Training Loss for validation set on EPOCH 7 : 0.046855011591460646\n",
    "Epoch 8: 100%|██████████| 335/335 [00:17<00:00, 18.98it/s, loss=0.0176]\n",
    "Mean Training Loss 0.03557930594282364\n",
    "Epoch 8: 100%|██████████| 41/41 [00:00<00:00, 73.56it/s, loss=nan]   \n",
    "Mean Training Loss for validation set on EPOCH 8 : 0.048434071335941555\n",
    "Epoch 9: 100%|██████████| 335/335 [00:17<00:00, 18.86it/s, loss=0.0375]\n",
    "Mean Training Loss 0.033892850247003246\n",
    "Epoch 9: 100%|██████████| 41/41 [00:00<00:00, 73.40it/s, loss=0.0291]\n",
    "Mean Training Loss for validation set on EPOCH 9 : 0.04664606033120214\n",
    "Test: 100%|██████████| 90/90 [00:01<00:00, 66.85it/s, loss=0.606] \n",
    "Mean Training Loss for test set : 0.15808176810152075\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ft_model.state_dict(), 'pt_m_2ynorm_v1_ft_v2_10Day_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this model we will use a method called autoregression. Autoregression is where predictions are based on previous values of the target variable.\n",
    "\n",
    "The core idea of autoregression is as follows:\n",
    "\n",
    "1. The model makes an initial prediction\n",
    "\n",
    "2. We feed this prediction back into our dataset\n",
    "\n",
    "3. Based on our prediction and previous values, we make another prediction.\n",
    "\n",
    "What we are doing is creating a feedback loop where we keep making predictions based off prior predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "NUM_INPUT = len(dataset.columns)\n",
    "NUM_TARGET = 2\n",
    "CONTEXT_LEN = 120\n",
    "PATCH_LEN = 20\n",
    "PRED_LEN = 10\n",
    "PATCH_STRD = int(PATCH_LEN * .35)\n",
    "MASK_TYPE = 'forecast'\n",
    "D_MODEL = 256\n",
    "NUM_HIDD_LAYERS = 4\n",
    "NUM_ATT_HEAD = 8\n",
    "ATT_DROP = .1\n",
    "FF_DROP = .2\n",
    "PATH_DROP = (NUM_HIDD_LAYERS - 2) * .05\n",
    "NUM_PATCH = [int(BATCH_SIZE * .4)]\n",
    "\n",
    "features_to_pred = ['^GSPC Close', '^DJI Close']\n",
    "\n",
    "# How many days to predict into the future\n",
    "PRED_LEN = 7 # One Month Prediction\n",
    "\n",
    "ft_config = PatchTSTConfig(\n",
    "    num_input_channels = NUM_INPUT,\n",
    "    num_targets = NUM_TARGET,\n",
    "    context_length = CONTEXT_LEN,\n",
    "    patch_length = PATCH_LEN,\n",
    "    stride = PATCH_STRD,\n",
    "    prediction_length=PRED_LEN,\n",
    "    d_model=D_MODEL,\n",
    "    num_hidden_layers=NUM_HIDD_LAYERS,\n",
    "    num_attention_heads=NUM_ATT_HEAD,\n",
    "    attention_dropout=ATT_DROP,\n",
    "    ff_dropout=FF_DROP,\n",
    "    path_dropout=PATH_DROP,\n",
    ")\n",
    "\n",
    "\n",
    "ft_model = PatchTSTForPrediction(ft_config)\n",
    "\n",
    "# Load in our model\n",
    "device = torch.device('cpu')\n",
    "state_dict = torch.load('pt_m_2ynorm_v1_ft_v1_model.bin')\n",
    "ft_model.load_state_dict(state_dict)\n",
    "\n",
    "# Get the normalized economic data\n",
    "pred_and_original_data = pd.read_csv('../data/economic_data_2_year_sliding_window.csv')\n",
    "\n",
    "pred_and_original_data[0:len(pred_and_original_data)]\n",
    "\n",
    "# Filter out data to only contain features needed and Date for merging\n",
    "pred_and_original_data = pred_and_original_data[['DATE']+features_to_pred]\n",
    "\n",
    "# How many predictions we will recursively make\n",
    "NUM_ITER = 3\n",
    "\n",
    "# Autoregression\n",
    "for day in range(NUM_ITER):\n",
    "    \n",
    "    # Drop the date for prediction\n",
    "    df_copy = pred_and_original_data.drop(['DATE'], axis=1)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Grab the last instance of CONTEXT_LEN days\n",
    "        features = df_copy.iloc[-CONTEXT_LEN:].values\n",
    "        \n",
    "        # Convert to proper format for model\n",
    "        features = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "\n",
    "        predictions = ft_model(past_values=features)\n",
    "        forcast = predictions.prediction_outputs\n",
    "    \n",
    "    # Convert predictions back to prior format\n",
    "    predictions = forcast.squeeze(0).numpy()\n",
    "    pred_np = predictions\n",
    "    \n",
    "    # Get the last date in the dataset\n",
    "    last_date = pd.to_datetime(pred_and_original_data['DATE'])\n",
    "    last_date = last_date[len(pred_and_original_data) - 1]\n",
    "\n",
    "    # Create date range for predictions\n",
    "    future_dates = pd.date_range(\n",
    "        start=last_date + timedelta(days=1),\n",
    "        periods=PRED_LEN,\n",
    "        freq='D'\n",
    "    )\n",
    "    \n",
    "    # Create a dataframe of our predictions with their corresponding dates\n",
    "    pred_df = pd.DataFrame(pred_np, columns=features_to_pred)\n",
    "    pred_df['DATE'] = future_dates\n",
    "\n",
    "    # Add predictions to dataset\n",
    "    pred_and_original_data = pd.concat([pred_and_original_data, pred_df], ignore_index=True)\n",
    "    pred_and_original_data['DATE'] = pd.to_datetime(pred_and_original_data['DATE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Predictions back to Base Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "original_df = pd.read_csv('../data/pre_norm_economic_data94_25.csv')\n",
    "original_df = original_df[['DATE'] + features_to_pred]\n",
    "original_df['DATE'] = pd.to_datetime(original_df['DATE'])\n",
    "\n",
    "WINDOW_SIZE = int(365 * 1.5) # Should be the same size from the one we used to normalize, REFER TO Gather_Historical_Data.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7665/7665 [00:06<00:00, 1160.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def norm_data_st(norm_df, df, col, window_size=90):\n",
    "    denorm_df = norm_df.copy()\n",
    "\n",
    "    # Offset for getting the proper index for our normalized data\n",
    "    offset = window_size\n",
    "\n",
    "    # Moving through each row starting at \n",
    "    # window_size and going to df length\n",
    "    for i in tqdm(range(len(norm_df))):\n",
    "        original_id = i + offset\n",
    "        \n",
    "        window_start = original_id - window_size\n",
    "\n",
    "        # Grabs data from past\n",
    "        time_window = df.iloc[window_start:original_id]\n",
    "\n",
    "        # Grab current time\n",
    "        current_time = norm_df.iloc[i:i+1]\n",
    "\n",
    "        # Create scaler on based on window\n",
    "        robust_scaler = RobustScaler()\n",
    "        robust_scaler.fit(time_window[col])\n",
    "\n",
    "        # Apply scaler to current index feature\n",
    "        denomralized_values = robust_scaler.inverse_transform(current_time[col])\n",
    "        denorm_df.loc[norm_df.index[i], col] = denomralized_values[0]\n",
    "    return denorm_df\n",
    "\n",
    "columns_to_unnormalize = pred_and_original_data.drop(['DATE'], axis=1).columns\n",
    "unnorm_df = norm_data_st(pred_and_original_data, original_df, columns_to_unnormalize,WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_time_frame_x2(year_start, year_end, df, x1, x2):\n",
    "    df_tf = df[(df['DATE'].dt.year >= year_start) & (df['DATE'].dt.year <= year_end)]\n",
    "\n",
    "    plt.plot(df_tf['DATE'], df_tf[x1], label=x1)\n",
    "    plt.plot(df_tf['DATE'], df_tf[x2], label=x2)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "def plot_time_frame_month(year_start, year_end, month_start, month_end, df1, df2, x1, x2):\n",
    "    df_tf1 = df1[(df1['DATE'].dt.year >= year_start) & (df1['DATE'].dt.year <= year_end)]\n",
    "    df_tf1 = df_tf1[(df_tf1['DATE'].dt.month >= month_start) & (df_tf1['DATE'].dt.month <= month_end)]\n",
    "    \n",
    "    df_tf2 = df2[(df2['DATE'].dt.year >= year_start) & (df2['DATE'].dt.year <= year_end)]\n",
    "    df_tf2 = df_tf2[(df_tf2['DATE'].dt.month >= month_start) & (df_tf2['DATE'].dt.month <= month_end)]\n",
    "    \n",
    "    plt.plot(df_tf2['DATE'], df_tf2[x1], label=f'{x1} Projection', linestyle='--')\n",
    "    plt.plot(df_tf2['DATE'], df_tf2[x2], label=f'{x2} Projection', linestyle='--')\n",
    "    \n",
    "    plt.plot(df_tf1['DATE'], df_tf1[x1], label=x1)\n",
    "    plt.plot(df_tf1['DATE'], df_tf1[x2], label=x2)\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set a tick for every day\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d'))\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()  # This helps prevent the rotated labels from being cut off\n",
    "    plt.show()\n",
    "    \n",
    "def plot_time_frame_year(year_start, year_end, df1, df2, x1, x2):\n",
    "    df_tf1 = df1[(df1['DATE'].dt.year >= year_start) & (df1['DATE'].dt.year <= year_end)]\n",
    "    \n",
    "    df_tf2 = df2[(df2['DATE'].dt.year >= year_start) & (df2['DATE'].dt.year <= year_end)]\n",
    "    \n",
    "    plt.plot(df_tf2['DATE'], df_tf2[x1], label=f'{x1} Projection', linestyle='--')\n",
    "    plt.plot(df_tf2['DATE'], df_tf2[x2], label=f'{x2} Projection', linestyle='--')\n",
    "    \n",
    "    plt.plot(df_tf1['DATE'], df_tf1[x1], label=x1)\n",
    "    plt.plot(df_tf1['DATE'], df_tf1[x2], label=x2)\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set a tick for every day\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d'))\n",
    "    \n",
    "    # plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.tight_layout()  # This helps prevent the rotated labels from being cut off\n",
    "    plt.show()\n",
    "    \n",
    "def plot_time_frame_year_x1(year_start, year_end, df1, df2, x1):\n",
    "    df_tf1 = df1[(df1['DATE'].dt.year >= year_start) & (df1['DATE'].dt.year <= year_end)]\n",
    "    \n",
    "    df_tf2 = df2[(df2['DATE'].dt.year >= year_start) & (df2['DATE'].dt.year <= year_end)]\n",
    "    \n",
    "    plt.plot(df_tf2['DATE'], df_tf2[x1], label=f'{x1} Projection', linestyle='--')\n",
    "    \n",
    "    plt.plot(df_tf1['DATE'], df_tf1[x1], label=x1)\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set a tick for every day\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d'))\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.tight_layout()  # This helps prevent the rotated labels from being cut off\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_df['DATE'] = pd.to_datetime(original_df['DATE'])\n",
    "\n",
    "plot_time_frame_year_x1(2025, 2025, original_df, unnorm_df, features_to_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
