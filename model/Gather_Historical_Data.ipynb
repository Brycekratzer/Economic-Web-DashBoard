{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Historical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notebook, we will gather historical data from 2000-2025 to be used for training the `PatchTST` time-series based transformer.\n",
    "Multiple methods will be shown and explained such as, data preprocessing, data cleaning, and normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import data from the FRED and yfinance that track multiple economic factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing The Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.NaN = np.nan # For pandas_ta\n",
    "import yfinance as yf\n",
    "import pandas_datareader as pdr\n",
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Tickers for stocks\n",
    "SP_TICKER = '^GSPC'\n",
    "DJ_TICKER = '^DJI'\n",
    "NAS_TICKER = '^IXIC'\n",
    "VIX_TICKER = '^VIX'\n",
    "\n",
    "# Stock Futures \n",
    "SPF_TICKER = 'ES=F'\n",
    "DJF_TICKER = 'YM=F'\n",
    "NASF_TICKER = 'NQ=F'\n",
    "\n",
    "# Define FRED data series codes\n",
    "CPI_CODE = 'CPIAUCSL'\n",
    "INTEREST_RATES_CODE = 'DFF'\n",
    "UNEMPLOYMENT_RATES_CODE = 'UNRATE'\n",
    "GDP_CODE = 'GDP'\n",
    "M2_CODE = 'WM2NS'\n",
    "INITIAL_CLAIMS_CODE = 'ICSA'\n",
    "JOB_OPENINGS_CONSTRUCTION_CODE = 'JTS2300JOL'\n",
    "JOB_OPENINGS_PRIVATE_CODE = 'JTS1000JOL'\n",
    "JOB_OPENINGS_NF_CODE = 'JTSJOL'\n",
    "PCE_CODE = 'PCE'\n",
    "CONSUMER_DEBT_CODE = 'REVOLSL'\n",
    "TDSP_CODE = 'TDSP'\n",
    "CDSP_CODE = 'CDSP'\n",
    "T10Y2Y_CODE = 'T10Y2Y'\n",
    "T10YFF_CODE = 'T10YFF'\n",
    "INDUSTRIAL_PROD_CODE = 'INDPRO'\n",
    "CAPACITY_PROD_CODE = 'TCU'\n",
    "RETAIL_SALES_CODE = 'RSAFS'\n",
    "PERSONAL_SAVINGS_CODE = 'PSAVERT'\n",
    "MORTGAGE_30_YEAR_CODE = 'MORTGAGE30US'\n",
    "NEW_PRIV_HOUSING_CODE = 'HOUST'\n",
    "PERSONAL_INCOME_CODE = 'PI'\n",
    "\n",
    "# Define Domain of Data\n",
    "START_DATE = '2002-04-05'\n",
    "END_DATE = '2025-04-07'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function acts as a way to grab data from the fred based on the given \n",
    "# code and domain\n",
    "def get_data_frame(fred_code, start_date, end_date):\n",
    "    df = pdr.get_data_fred(fred_code, start=start_date, end=end_date)\n",
    "    df = df.reset_index()\n",
    "    df[fred_code] = df[fred_code].ffill().bfill()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function acts as a way to grab data for stocks based on the given \n",
    "# ticker and domain\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    copy_df = yf.download(ticker, start=start_date, end=end_date).ffill().bfill()\n",
    "    copy_df = copy_df.reset_index()\n",
    "    copy_df = copy_df[['Date', 'High', 'Low', 'Open', 'Close', 'Volume']]\n",
    "    copy_df.columns = copy_df.columns.get_level_values(0)\n",
    "    copy_df = copy_df.rename(columns={\n",
    "           'Date'  : 'DATE',\n",
    "           'High'  : f'{ticker} High', \n",
    "           'Low'   : f'{ticker} Low', \n",
    "           'Open'  : f'{ticker} Open', \n",
    "           'Close' : f'{ticker} Close', \n",
    "           'Volume': f'{ticker} Volume'\n",
    "    })\n",
    "    return copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# S&P data\n",
    "SP500 = get_stock_data(SP_TICKER, START_DATE, END_DATE)\n",
    "\n",
    "# Dow Jones data\n",
    "Dow_Jones = get_stock_data(DJ_TICKER, START_DATE, END_DATE)\n",
    "\n",
    "# Nasdaq data\n",
    "Nas = get_stock_data(NAS_TICKER, START_DATE, END_DATE)\n",
    "\n",
    "# VIX data\n",
    "Vix = get_stock_data(VIX_TICKER, START_DATE, END_DATE)\n",
    "\n",
    "# S&P Futures\n",
    "SPF = get_stock_data(SPF_TICKER, START_DATE, END_DATE)\n",
    "\n",
    "# Dow Jones data\n",
    "DWF = get_stock_data(DJF_TICKER, START_DATE, END_DATE)\n",
    "\n",
    "# Nasdaq data\n",
    "NASF = get_stock_data(NASF_TICKER, START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataFrames for FRED Data\n",
    "cpi_df = get_data_frame(CPI_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "interest_rates_df = get_data_frame(INTEREST_RATES_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "unemployment_rates_df = get_data_frame(UNEMPLOYMENT_RATES_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "gdp_df = get_data_frame(GDP_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "m2_df = get_data_frame(M2_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "initial_claims_df = get_data_frame(INITIAL_CLAIMS_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "job_openings_construction_df = get_data_frame(JOB_OPENINGS_CONSTRUCTION_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "job_openings_private_df = get_data_frame(JOB_OPENINGS_PRIVATE_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "job_openings_nf_df = get_data_frame(JOB_OPENINGS_NF_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "pce_df = get_data_frame(PCE_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "consumer_debt_df = get_data_frame(CONSUMER_DEBT_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "tdsp_df = get_data_frame(TDSP_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "cdsp_df = get_data_frame(CDSP_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "t10y2y_df = get_data_frame(T10Y2Y_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "t10yff_df = get_data_frame(T10YFF_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "industrial_prod_df = get_data_frame(INDUSTRIAL_PROD_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "capacity_prod_df = get_data_frame(CAPACITY_PROD_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "retail_sales_df = get_data_frame(RETAIL_SALES_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "personal_savings_df = get_data_frame(PERSONAL_SAVINGS_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "mortgage_30_year_df = get_data_frame(MORTGAGE_30_YEAR_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "new_priv_housing_df = get_data_frame(NEW_PRIV_HOUSING_CODE, start_date=START_DATE, end_date=END_DATE)\n",
    "personal_income_df = get_data_frame(PERSONAL_INCOME_CODE, start_date=START_DATE, end_date=END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will adjust values for inflation, and put all data frames into one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflation Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inflation is defined in multiple ways, but we will use the following formula to calculate inflation using the average CPI per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Adjusted Amount} = \\text{Past Year Dollar Amount} \\times \\frac{\\text{Current Year CPI}}{\\text{Past Year CPI}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average CPI for each year to calculate inflation\n",
    "CPI_yearly_amount = cpi_df.groupby(cpi_df['DATE'].dt.year)[CPI_CODE].mean().reset_index()\n",
    "\n",
    "# For later calculations\n",
    "CPI_2025 = CPI_yearly_amount[CPI_yearly_amount['DATE'] == 2025][CPI_CODE].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula that returns a DataFrame that adjust contents for inflation\n",
    "def adjust_for_inflation(df, features, CPI_yearly_amount):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Get Year for both DF's\n",
    "    df['Year'] = df['DATE'].dt.year\n",
    "    CPI_yearly_amount = CPI_yearly_amount.rename(columns={'DATE' : 'Year'})\n",
    "\n",
    "    # Add a new column to the df that has the CPI for each year\n",
    "    df = pd.merge_asof(df, CPI_yearly_amount, on='Year', direction='forward')\n",
    "                        \n",
    "    # Adjust each value for inflation       \n",
    "    for feature in features:\n",
    "        df[feature] = df[feature] * (CPI_2025 / df[CPI_CODE])\n",
    "        \n",
    "    df = df.drop([CPI_CODE, 'Year'], axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500 = adjust_for_inflation(SP500, [f'{SP_TICKER} High', f'{SP_TICKER} Low', f'{SP_TICKER} Open', f'{SP_TICKER} Close'], CPI_yearly_amount)\n",
    "Dow_Jones = adjust_for_inflation(Dow_Jones, [f'{DJ_TICKER} High', f'{DJ_TICKER} Low', f'{DJ_TICKER} Open', f'{DJ_TICKER} Close'], CPI_yearly_amount)\n",
    "Nas_Inflation = adjust_for_inflation(Nas, [f'{NAS_TICKER} High', f'{NAS_TICKER} Low', f'{NAS_TICKER} Open', f'{NAS_TICKER} Close'], CPI_yearly_amount)\n",
    "SPF = adjust_for_inflation(SPF, [f'{SPF_TICKER} High', f'{SPF_TICKER} Low', f'{SPF_TICKER} Open', f'{SPF_TICKER} Close'], CPI_yearly_amount)\n",
    "DWF = adjust_for_inflation(DWF, [f'{DJF_TICKER} High', f'{DJF_TICKER} Low', f'{DJF_TICKER} Open', f'{DJF_TICKER} Close'], CPI_yearly_amount)\n",
    "NASF = adjust_for_inflation(NASF, [f'{NASF_TICKER} High', f'{NASF_TICKER} Low', f'{NASF_TICKER} Open', f'{NASF_TICKER} Close'], CPI_yearly_amount)\n",
    "pce_df = adjust_for_inflation(pce_df, [PCE_CODE], CPI_yearly_amount)\n",
    "consumer_debt_df = adjust_for_inflation(consumer_debt_df, [CONSUMER_DEBT_CODE], CPI_yearly_amount)\n",
    "retail_sales_df = adjust_for_inflation(retail_sales_df, [RETAIL_SALES_CODE], CPI_yearly_amount)\n",
    "personal_income_df = adjust_for_inflation(personal_income_df, [PERSONAL_INCOME_CODE], CPI_yearly_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all Data into One DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will combine all data based on the Date. For the different frequency of data we will fill missing gaps by filling forward with the previously received data\n",
    "\n",
    "- For example:\n",
    "\n",
    "    - GDP only updates quarterly while S&P 500 data is updates daily. The GDP will have missing values for each day, besides when a new report came out. \n",
    "    To adjust for this we will just fill the missing spots with what the previous GDP record was. \n",
    "    - Example below demonstraights this:\n",
    "\n",
    "    | S&P Close | GDP |\n",
    "    |----------|----------|\n",
    "    | 5010 |   15.3 |\n",
    "    | 5070 | NaN |\n",
    "    | 5040 | NaN |\n",
    "\n",
    "    **Gets Converted Too**\n",
    "\n",
    "    | S&P Close | GDP |\n",
    "    |----------|----------|\n",
    "    | 5010 |   15.3 |\n",
    "    | 5070 | 15.3 |\n",
    "    | 5040 | 15.3 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function merges the two dataFrames based on the date. Basically adds the col_name to the main_df\n",
    "def add_data_based_on_date(main_df, new_data, col_name):\n",
    "    return pd.merge_asof(\n",
    "            main_df, \n",
    "            new_data,\n",
    "            on='DATE',\n",
    "            direction='forward' # How we fill the gaps when adding content\n",
    "        )[col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Stock Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Add date for merging\n",
    "combined_df['DATE'] = SP500['DATE']\n",
    "\n",
    "# Adding Stock Markets\n",
    "combined_df[f'{SP_TICKER} Close'] = add_data_based_on_date(combined_df, SP500, f'{SP_TICKER} Close')\n",
    "combined_df[f'{SP_TICKER} High'] = add_data_based_on_date(combined_df, SP500, f'{SP_TICKER} High')\n",
    "combined_df[f'{SP_TICKER} Low'] = add_data_based_on_date(combined_df, SP500, f'{SP_TICKER} Low')\n",
    "combined_df[f'{SP_TICKER} Open'] = add_data_based_on_date(combined_df, SP500, f'{SP_TICKER} Open')\n",
    "combined_df[f'{SP_TICKER} Volume'] = add_data_based_on_date(combined_df, SP500, f'{SP_TICKER} Volume')\n",
    "\n",
    "combined_df[f'{DJ_TICKER} Close'] = add_data_based_on_date(combined_df, Dow_Jones, f'{DJ_TICKER} Close')\n",
    "combined_df[f'{DJ_TICKER} High'] = add_data_based_on_date(combined_df, Dow_Jones, f'{DJ_TICKER} High')\n",
    "combined_df[f'{DJ_TICKER} Low'] = add_data_based_on_date(combined_df, Dow_Jones, f'{DJ_TICKER} Low')\n",
    "combined_df[f'{DJ_TICKER} Open'] = add_data_based_on_date(combined_df, Dow_Jones, f'{DJ_TICKER} Open')\n",
    "combined_df[f'{DJ_TICKER} Volume'] = add_data_based_on_date(combined_df, Dow_Jones, f'{DJ_TICKER} Volume')\n",
    "\n",
    "combined_df[f'{NAS_TICKER} Close'] = add_data_based_on_date(combined_df, Nas, f'{NAS_TICKER} Close')\n",
    "combined_df[f'{NAS_TICKER} High'] = add_data_based_on_date(combined_df, Nas, f'{NAS_TICKER} High')\n",
    "combined_df[f'{NAS_TICKER} Low'] = add_data_based_on_date(combined_df, Nas, f'{NAS_TICKER} Low')\n",
    "combined_df[f'{NAS_TICKER} Open'] = add_data_based_on_date(combined_df, Nas, f'{NAS_TICKER} Open')\n",
    "combined_df[f'{NAS_TICKER} Volume'] = add_data_based_on_date(combined_df, Nas, f'{NAS_TICKER} Volume')\n",
    "\n",
    "combined_df[f'{VIX_TICKER} Close'] = add_data_based_on_date(combined_df, Vix, f'{VIX_TICKER} Close')\n",
    "combined_df[f'{VIX_TICKER} High'] = add_data_based_on_date(combined_df, Vix, f'{VIX_TICKER} High')\n",
    "combined_df[f'{VIX_TICKER} Low'] = add_data_based_on_date(combined_df, Vix, f'{VIX_TICKER} Low')\n",
    "combined_df[f'{VIX_TICKER} Open'] = add_data_based_on_date(combined_df, Vix, f'{VIX_TICKER} Open')\n",
    "combined_df[f'{VIX_TICKER} Volume'] = add_data_based_on_date(combined_df, Vix, f'{VIX_TICKER} Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Stock Futures\n",
    "# Adding Stock Markets\n",
    "combined_df[f'{SPF_TICKER} Close'] = add_data_based_on_date(combined_df, SPF, f'{SPF_TICKER} Close')\n",
    "combined_df[f'{SPF_TICKER} High'] = add_data_based_on_date(combined_df, SPF, f'{SPF_TICKER} High')\n",
    "combined_df[f'{SPF_TICKER} Low'] = add_data_based_on_date(combined_df, SPF, f'{SPF_TICKER} Low')\n",
    "combined_df[f'{SPF_TICKER} Open'] = add_data_based_on_date(combined_df, SPF, f'{SPF_TICKER} Open')\n",
    "combined_df[f'{SPF_TICKER} Volume'] = add_data_based_on_date(combined_df, SPF, f'{SPF_TICKER} Volume')\n",
    "\n",
    "combined_df[f'{DJF_TICKER} Close'] = add_data_based_on_date(combined_df, DWF, f'{DJF_TICKER} Close')\n",
    "combined_df[f'{DJF_TICKER} High'] = add_data_based_on_date(combined_df, DWF, f'{DJF_TICKER} High')\n",
    "combined_df[f'{DJF_TICKER} Low'] = add_data_based_on_date(combined_df, DWF, f'{DJF_TICKER} Low')\n",
    "combined_df[f'{DJF_TICKER} Open'] = add_data_based_on_date(combined_df, DWF, f'{DJF_TICKER} Open')\n",
    "combined_df[f'{DJF_TICKER} Volume'] = add_data_based_on_date(combined_df, DWF, f'{DJF_TICKER} Volume')\n",
    "\n",
    "combined_df[f'{NASF_TICKER} Close'] = add_data_based_on_date(combined_df, NASF, f'{NASF_TICKER} Close')\n",
    "combined_df[f'{NASF_TICKER} High'] = add_data_based_on_date(combined_df, NASF, f'{NASF_TICKER} High')\n",
    "combined_df[f'{NASF_TICKER} Low'] = add_data_based_on_date(combined_df, NASF, f'{NASF_TICKER} Low')\n",
    "combined_df[f'{NASF_TICKER} Open'] = add_data_based_on_date(combined_df, NASF, f'{NASF_TICKER} Open')\n",
    "combined_df[f'{NASF_TICKER} Volume'] = add_data_based_on_date(combined_df, NASF, f'{NASF_TICKER} Volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Monetary Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[f'{CPI_CODE}'] = add_data_based_on_date(combined_df, cpi_df, CPI_CODE)\n",
    "combined_df[f'{INTEREST_RATES_CODE}'] = add_data_based_on_date(combined_df, interest_rates_df, INTEREST_RATES_CODE)\n",
    "combined_df[f'{UNEMPLOYMENT_RATES_CODE}'] = add_data_based_on_date(combined_df, unemployment_rates_df, UNEMPLOYMENT_RATES_CODE)\n",
    "combined_df[f'{GDP_CODE}'] = add_data_based_on_date(combined_df, gdp_df, GDP_CODE)\n",
    "combined_df[f'{M2_CODE}'] = add_data_based_on_date(combined_df, m2_df, M2_CODE)\n",
    "combined_df[f'{INITIAL_CLAIMS_CODE}'] = add_data_based_on_date(combined_df, initial_claims_df, INITIAL_CLAIMS_CODE)\n",
    "combined_df[f'{JOB_OPENINGS_CONSTRUCTION_CODE}'] = add_data_based_on_date(combined_df, job_openings_construction_df, JOB_OPENINGS_CONSTRUCTION_CODE)\n",
    "combined_df[f'{JOB_OPENINGS_PRIVATE_CODE}'] = add_data_based_on_date(combined_df, job_openings_private_df, JOB_OPENINGS_PRIVATE_CODE)\n",
    "combined_df[f'{JOB_OPENINGS_NF_CODE}'] = add_data_based_on_date(combined_df, job_openings_nf_df, JOB_OPENINGS_NF_CODE)\n",
    "combined_df[f'{PCE_CODE}'] = add_data_based_on_date(combined_df, pce_df, PCE_CODE)\n",
    "combined_df[f'{CONSUMER_DEBT_CODE}'] = add_data_based_on_date(combined_df, consumer_debt_df, CONSUMER_DEBT_CODE)\n",
    "combined_df[f'{TDSP_CODE}'] = add_data_based_on_date(combined_df, tdsp_df, TDSP_CODE)\n",
    "combined_df[f'{CDSP_CODE}'] = add_data_based_on_date(combined_df, cdsp_df, CDSP_CODE)\n",
    "combined_df[f'{T10Y2Y_CODE}'] = add_data_based_on_date(combined_df, t10y2y_df, T10Y2Y_CODE)\n",
    "combined_df[f'{T10YFF_CODE}'] = add_data_based_on_date(combined_df, t10yff_df, T10YFF_CODE)\n",
    "combined_df[f'{INDUSTRIAL_PROD_CODE}'] = add_data_based_on_date(combined_df, industrial_prod_df, INDUSTRIAL_PROD_CODE)\n",
    "combined_df[f'{CAPACITY_PROD_CODE}'] = add_data_based_on_date(combined_df, capacity_prod_df, CAPACITY_PROD_CODE)\n",
    "combined_df[f'{RETAIL_SALES_CODE}'] = add_data_based_on_date(combined_df, retail_sales_df, RETAIL_SALES_CODE)\n",
    "combined_df[f'{PERSONAL_SAVINGS_CODE}'] = add_data_based_on_date(combined_df, personal_savings_df, PERSONAL_SAVINGS_CODE)\n",
    "combined_df[f'{MORTGAGE_30_YEAR_CODE}'] = add_data_based_on_date(combined_df, mortgage_30_year_df, MORTGAGE_30_YEAR_CODE)\n",
    "combined_df[f'{NEW_PRIV_HOUSING_CODE}'] = add_data_based_on_date(combined_df, new_priv_housing_df, NEW_PRIV_HOUSING_CODE)\n",
    "combined_df[f'{PERSONAL_INCOME_CODE}'] = add_data_based_on_date(combined_df, personal_income_df, PERSONAL_INCOME_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To account for any missing values\n",
    "combined_df = combined_df.ffill()\n",
    "combined_df.to_csv('../data/model_data/PreNorm_Full_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Window Based Normlization (Time-Series Based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we will apply normilzation is as follows:\n",
    "\n",
    "- Using scikit learns `RobustScaler` for better outlier handling for features by:\n",
    "    - Calculating the scalar using the IQR, the range between the 75th and 25th percentiles\n",
    "        - Using the standard divation is more prone to outliers\n",
    "        - Using the minimum and maximum values could be outliers\n",
    "        - **Uses *Interquartile range* that includes where most values fall**\n",
    "    - Also uses the median rather than the mean\n",
    "        - Using the mean is heavily influenced by outliers\n",
    "        - X_min could be the outlier itself\n",
    "        - **Uses the *median* which isn't effected by outliers as much**\n",
    "\n",
    "- Will only scale based on **Past** data.\n",
    "    - `window_size` will be our scale for how far we want to look back in time for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "def norm_data_st(df, col, window_size=90):\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    # Moving through each row starting at \n",
    "    # window_size and going to df length\n",
    "    for i in tqdm(range(window_size, len(df))):\n",
    "\n",
    "        # Grabs data from past\n",
    "        time_window = df.iloc[i-window_size:i]\n",
    "\n",
    "        # Grab current time\n",
    "        current_time = df.iloc[i:i+1]\n",
    "\n",
    "        # Create scaler on based on window\n",
    "        robust_scaler = RobustScaler()\n",
    "        robust_scaler.fit(time_window[col])\n",
    "\n",
    "        # Apply scaler to current index feature\n",
    "        scaled_values = robust_scaler.transform(current_time[col])\n",
    "        df_scaled.loc[df.index[i], col] = scaled_values[0]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5242 [00:00<?, ?it/s]/var/folders/dz/ls7s785d52j9jwwk1cpbk5fr0000gn/T/ipykernel_2801/3585957934.py:23: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.7758399118780981' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_scaled.loc[df.index[i], col] = scaled_values[0]\n",
      "/var/folders/dz/ls7s785d52j9jwwk1cpbk5fr0000gn/T/ipykernel_2801/3585957934.py:23: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.8358186244674376' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_scaled.loc[df.index[i], col] = scaled_values[0]\n",
      "/var/folders/dz/ls7s785d52j9jwwk1cpbk5fr0000gn/T/ipykernel_2801/3585957934.py:23: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.5332708303283812' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_scaled.loc[df.index[i], col] = scaled_values[0]\n",
      "/var/folders/dz/ls7s785d52j9jwwk1cpbk5fr0000gn/T/ipykernel_2801/3585957934.py:23: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.09648248404791891' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_scaled.loc[df.index[i], col] = scaled_values[0]\n",
      "/var/folders/dz/ls7s785d52j9jwwk1cpbk5fr0000gn/T/ipykernel_2801/3585957934.py:23: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9122467986030268' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_scaled.loc[df.index[i], col] = scaled_values[0]\n",
      "/var/folders/dz/ls7s785d52j9jwwk1cpbk5fr0000gn/T/ipykernel_2801/3585957934.py:23: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.08270339270399106' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_scaled.loc[df.index[i], col] = scaled_values[0]\n",
      "100%|██████████| 5242/5242 [00:26<00:00, 196.75it/s]\n"
     ]
    }
   ],
   "source": [
    "normalized_df = combined_df.copy()\n",
    "\n",
    "# Our data will be normalized on a 2 year scale\n",
    "WINDOW_SIZE = int(365 * 1.5) \n",
    "\n",
    "# Remove date from normalization\n",
    "columns_to_normalize = combined_df.columns[1:len(combined_df)]\n",
    "\n",
    "normalized_df = norm_data_st(normalized_df, columns_to_normalize, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to drop the first 1.5 years as they weren't normalized\n",
    "normalized_df = normalized_df[WINDOW_SIZE:len(normalized_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.to_csv('../data/model_data/PostNorm_Full_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
